from easydict import EasyDict
import os

# ==============================================================
# GAT StochasticMuZero è»¢ç§»å­¦ç¿’è¨­å®š (3Ã—3 â†’ 4Ã—4)
# ==============================================================
# 1. ã¾ãš3Ã—3ã§å­¦ç¿’
# 2. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’4Ã—4ã«è»¢ç§»
# ==============================================================

env_id = 'game_2048'
action_space_size = 4
use_ture_chance_label_in_chance_encoder = True

# åŸºæœ¬è¨­å®š
collector_env_num = 8
n_episode = 8
evaluator_env_num = 3
num_simulations = 100
update_per_collect = 200
batch_size = 512
max_env_step = int(1e6)
reanalyze_ratio = 0.
num_of_possible_chance_tile = 2

# GATè¨­å®šï¼ˆè»¢ç§»å­¦ç¿’ã§å…±é€šï¼‰
num_heads = 4
hidden_channels = 64
num_gat_layers = 3
state_dim = 256
dropout = 0.1

# StochasticMuZero specific parameters
chance_encoder_num_layers = 2
afterstate_reward_layers = 2
stochastic_loss_weight = 1.0

# ==============================================================
# PHASE 1: 3Ã—3ã‚°ãƒªãƒƒãƒ‰ã§ã®å­¦ç¿’è¨­å®š
# ==============================================================
def get_3x3_config():
    grid_size = 3
    chance_space_size = (grid_size ** 2) * num_of_possible_chance_tile  # 18
    
    config_3x3 = dict(
        exp_name=f'data_gat_transfer/phase1_3x3_gat_stochastic_ns{num_simulations}_upc{update_per_collect}_rer{reanalyze_ratio}_bs{batch_size}_heads{num_heads}_seed0',
        env=dict(
            stop_value=int(1e6),
            env_id=env_id,
            obs_shape=(16, grid_size, grid_size),
            obs_type='dict_encoded_board',
            num_of_possible_chance_tile=num_of_possible_chance_tile,
            collector_env_num=collector_env_num,
            evaluator_env_num=1,
            n_evaluator_episode=1,
            manager=dict(shared_memory=False, ),
        ),
        policy=dict(
            type='stochastic_muzero',
            model=dict(
                type='GATStochasticMuZeroModel',
                model_type='gat',
                model='gat_stochastic',
                observation_shape=(16, grid_size, grid_size),
                image_channel=16,
                action_space_size=action_space_size,
                chance_space_size=chance_space_size,
                frame_stack_num=1,
                
                # GAT parameters
                num_heads=num_heads,
                hidden_channels=hidden_channels,
                num_gat_layers=num_gat_layers,
                state_dim=state_dim,
                dropout=dropout,
                
                # StochasticMuZero parameters
                chance_encoder_num_layers=chance_encoder_num_layers,
                afterstate_reward_layers=afterstate_reward_layers,
                
                # Standard parameters
                value_head_channels=16,
                policy_head_channels=16,
                value_head_hidden_channels=[32],
                policy_head_hidden_channels=[32],
                
                flatten_input_size_for_value_head=state_dim,
                flatten_input_size_for_policy_head=state_dim,
                
                reward_support_size=601,
                value_support_size=601,
                categorical_distribution=True,
                last_linear_layer_init_zero=True,
                state_norm=False,
                self_supervised_learning_loss=True,
            ),
            model_path=None,
            use_ture_chance_label_in_chance_encoder=use_ture_chance_label_in_chance_encoder,
            cuda=True,
            game_segment_length=200,
            update_per_collect=update_per_collect,
            batch_size=batch_size,
            td_steps=10,
            discount_factor=0.999,
            manual_temperature_decay=True,
            optim_type='Adam',
            piecewise_decay_lr_scheduler=False,
            learning_rate=0.003,
            weight_decay=1e-4,
            num_simulations=num_simulations,
            reanalyze_ratio=reanalyze_ratio,
            ssl_loss_weight=0,
            stochastic_loss_weight=stochastic_loss_weight,
            n_episode=n_episode,
            eval_freq=int(2e5),  # çŸ­ã‚ã«è¨­å®šã—ã¦è»¢ç§»ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æ—©ã‚ã‚‹
            replay_buffer_size=int(1e6),
            collector_env_num=collector_env_num,
            evaluator_env_num=1,
        ),
    )
    return EasyDict(config_3x3)

# ==============================================================
# PHASE 2: 4Ã—4ã‚°ãƒªãƒƒãƒ‰ã¸ã®è»¢ç§»å­¦ç¿’è¨­å®š
# ==============================================================
def get_4x4_transfer_config(pretrained_model_path):
    grid_size = 4
    chance_space_size = (grid_size ** 2) * num_of_possible_chance_tile  # 32
    
    config_4x4 = dict(
        exp_name=f'data_gat_transfer/phase2_4x4_transfer_gat_stochastic_ns{num_simulations}_upc{update_per_collect}_rer{reanalyze_ratio}_bs{batch_size}_heads{num_heads}_seed0',
        env=dict(
            stop_value=int(1e6),
            env_id=env_id,
            obs_shape=(16, grid_size, grid_size),
            obs_type='dict_encoded_board',
            num_of_possible_chance_tile=num_of_possible_chance_tile,
            collector_env_num=collector_env_num,
            evaluator_env_num=1,
            n_evaluator_episode=1,
            manager=dict(shared_memory=False, ),
        ),
        policy=dict(
            type='stochastic_muzero',
            model=dict(
                type='GATStochasticMuZeroModel',
                model_type='gat',
                model='gat_stochastic',
                observation_shape=(16, grid_size, grid_size),
                image_channel=16,
                action_space_size=action_space_size,
                chance_space_size=chance_space_size,
                frame_stack_num=1,
                
                # GAT parameters (åŒã˜è¨­å®šã‚’ç¶­æŒ)
                num_heads=num_heads,
                hidden_channels=hidden_channels,
                num_gat_layers=num_gat_layers,
                state_dim=state_dim,
                dropout=dropout,
                
                # StochasticMuZero parameters
                chance_encoder_num_layers=chance_encoder_num_layers,
                afterstate_reward_layers=afterstate_reward_layers,
                
                # Standard parameters
                value_head_channels=16,
                policy_head_channels=16,
                value_head_hidden_channels=[32],
                policy_head_hidden_channels=[32],
                
                flatten_input_size_for_value_head=state_dim,
                flatten_input_size_for_policy_head=state_dim,
                
                reward_support_size=601,
                value_support_size=601,
                categorical_distribution=True,
                last_linear_layer_init_zero=True,
                state_norm=False,
                self_supervised_learning_loss=True,
            ),
            model_path=pretrained_model_path,  # 3Ã—3ã§å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
            use_ture_chance_label_in_chance_encoder=use_ture_chance_label_in_chance_encoder,
            cuda=True,
            game_segment_length=200,
            update_per_collect=update_per_collect,
            batch_size=batch_size,
            td_steps=10,
            discount_factor=0.999,
            manual_temperature_decay=True,
            optim_type='Adam',
            piecewise_decay_lr_scheduler=False,
            learning_rate=0.001,  # è»¢ç§»å­¦ç¿’ã§ã¯å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹
            weight_decay=1e-4,
            num_simulations=num_simulations,
            reanalyze_ratio=reanalyze_ratio,
            ssl_loss_weight=0,
            stochastic_loss_weight=stochastic_loss_weight,
            n_episode=n_episode,
            eval_freq=int(1e6),
            replay_buffer_size=int(1e6),
            collector_env_num=collector_env_num,
            evaluator_env_num=1,
        ),
    )
    return EasyDict(config_4x4)

# Create config
create_config = dict(
    env=dict(
        type='game_2048',
        import_names=['zoo.game_2048.envs.game_2048_env'],
    ),
    env_manager=dict(type='subprocess'),
    policy=dict(
        type='stochastic_muzero',
        import_names=['lzero.policy.stochastic_muzero'],
    ),
    model=dict(
        type='GATStochasticMuZeroModel',
        import_names=['lzero.model.gat_stochastic_muzero_model'],
    ),
)
create_config = EasyDict(create_config)

if __name__ == "__main__":
    from lzero.entry import train_muzero
    import torch
    import logging
    import glob
    
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 80)
    print("GAT STOCHASTIC MUZERO è»¢ç§»å­¦ç¿’")
    print("3Ã—3 â†’ 4Ã—4 ã‚°ãƒªãƒƒãƒ‰è»¢ç§»")
    print("=" * 80)
    
    # PHASE 1: 3Ã—3ã§å­¦ç¿’
    print("\nğŸ”„ PHASE 1: 3Ã—3ã‚°ãƒªãƒƒãƒ‰ã§ã®å­¦ç¿’é–‹å§‹")
    print("=" * 50)
    
    config_3x3 = get_3x3_config()
    print(f"âœ“ 3Ã—3è¨­å®š: chance_space_size={config_3x3.policy.model.chance_space_size}")
    print(f"âœ“ å®Ÿé¨“å: {config_3x3.exp_name}")
    
    # 3Ã—3ã§ã®å­¦ç¿’å®Ÿè¡Œ
    print("ğŸš€ 3Ã—3ã‚°ãƒªãƒƒãƒ‰ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    train_muzero([config_3x3, create_config], seed=0, model_path=None, max_env_step=int(5e5))  # çŸ­ã‚ã«è¨­å®š
    
    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹
    print("\nğŸ” å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¤œç´¢...")
    model_search_pattern = f"./data_gat_transfer/phase1_3x3_**/ckpt_*.pth.tar"
    model_files = glob.glob(model_search_pattern, recursive=True)
    
    if not model_files:
        print("âŒ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Phase 1ãŒæ­£å¸¸ã«å®Œäº†ã—ã¦ã„ã¾ã›ã‚“ã€‚")
        exit(1)
    
    # æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
    latest_model = max(model_files, key=os.path.getctime)
    print(f"âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç™ºè¦‹: {latest_model}")
    
    # PHASE 2: 4Ã—4ã¸ã®è»¢ç§»å­¦ç¿’
    print("\nğŸ”„ PHASE 2: 4Ã—4ã‚°ãƒªãƒƒãƒ‰ã¸ã®è»¢ç§»å­¦ç¿’é–‹å§‹")
    print("=" * 50)
    
    config_4x4 = get_4x4_transfer_config(latest_model)
    print(f"âœ“ 4Ã—4è¨­å®š: chance_space_size={config_4x4.policy.model.chance_space_size}")
    print(f"âœ“ å®Ÿé¨“å: {config_4x4.exp_name}")
    print(f"âœ“ ãƒ—ãƒªãƒˆãƒ¬ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«: {latest_model}")
    
    # è»¢ç§»å­¦ç¿’ã®ãŸã‚ã®ç‰¹åˆ¥ãªè¨­å®š
    print("\nğŸ“‹ è»¢ç§»å­¦ç¿’ã®ç‰¹åˆ¥è¨­å®š:")
    print(f"  - å­¦ç¿’ç‡: {config_4x4.policy.learning_rate} (å…ƒã®å­¦ç¿’ç‡ã‚ˆã‚Šä½ä¸‹)")
    print(f"  - ãƒ—ãƒªãƒˆãƒ¬ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«: ã‚ã‚Š")
    print(f"  - GATè¨­å®š: 3Ã—3ã¨åŒã˜ (è»¢ç§»å¯èƒ½)")
    
    # 4Ã—4ã§ã®è»¢ç§»å­¦ç¿’å®Ÿè¡Œ
    print("ğŸš€ 4Ã—4ã‚°ãƒªãƒƒãƒ‰ã§ã®è»¢ç§»å­¦ç¿’é–‹å§‹...")
    train_muzero([config_4x4, create_config], seed=0, model_path=latest_model, max_env_step=max_env_step)
    
    print("\nğŸ‰ è»¢ç§»å­¦ç¿’å®Œäº†ï¼")
    print("=" * 80)
