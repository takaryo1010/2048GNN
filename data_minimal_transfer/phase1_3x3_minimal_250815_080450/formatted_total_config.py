from easydict import EasyDict

main_config = dict(
    exp_name='data_minimal_transfer/phase1_3x3_minimal_250815_080450',
    env=dict(
        manager=dict(
            episode_num=float('inf'),
            max_retry=1,
            step_timeout=None,
            auto_reset=True,
            reset_timeout=None,
            retry_type='reset',
            retry_waiting_time=0.1,
            shared_memory=False,
            copy_on_get=True,
            context='fork',
            wait_num=float('inf'),
            step_wait_timeout=None,
            connect_timeout=60,
            reset_inplace=False,
            cfg_type='SyncSubprocessEnvManagerDict',
            type='subprocess',
        ),
        stop_value=1000000,
        n_evaluator_episode=1,
        env_id='game_2048',
        render_mode=None,
        replay_format='gif',
        replay_name_suffix='eval',
        replay_path=None,
        act_scale=True,
        channel_last=False,
        obs_type='dict_encoded_board',
        reward_normalize=False,
        reward_norm_scale=100,
        reward_type='raw',
        max_tile=65536,
        delay_reward_step=0,
        prob_random_agent=0.0,
        max_episode_steps=1000000,
        is_collect=True,
        ignore_legal_actions=True,
        need_flatten=False,
        num_of_possible_chance_tile=2,
        possible_tiles=[2 4],
        tile_probabilities=[0.9 0.1],
        cfg_type='Game2048EnvDict',
        obs_shape=(16, 3, 3),
        collector_env_num=1,
        evaluator_env_num=1,
    ),
    policy=dict(
        model=dict(
            model_type='gat',
            continuous_action_space=False,
            observation_shape=(16, 3, 3),
            self_supervised_learning_loss=True,
            categorical_distribution=True,
            image_channel=16,
            frame_stack_num=1,
            num_res_blocks=1,
            num_channels=64,
            support_scale=300,
            bias=True,
            discrete_action_encoding_type='one_hot',
            res_connection_in_dynamics=True,
            norm_type='BN',
            analysis_sim_norm=False,
            analysis_dormant_ratio=False,
            harmony_balance=False,
            chance_space_size=18,
            type='GATStochasticMuZeroModel',
            model='gat_stochastic',
            action_space_size=4,
            num_heads=1,
            hidden_channels=16,
            num_gat_layers=1,
            state_dim=64,
            dropout=0.0,
            chance_encoder_num_layers=1,
            afterstate_reward_layers=1,
            value_head_channels=4,
            policy_head_channels=4,
            value_head_hidden_channels=[8],
            policy_head_hidden_channels=[8],
            flatten_input_size_for_value_head=64,
            flatten_input_size_for_policy_head=64,
            reward_support_size=601,
            value_support_size=601,
            last_linear_layer_init_zero=True,
            state_norm=False,
        ),
        learn=dict(
            learner=dict(
                train_iterations=1000000000,
                dataloader=dict(
                    num_workers=0,
                ),
                log_policy=True,
                hook=dict(
                    load_ckpt_before_run='',
                    log_show_after_iter=100,
                    save_ckpt_after_iter=10000,
                    save_ckpt_after_run=True,
                ),
                cfg_type='BaseLearnerDict',
            ),
            resume_training=False,
        ),
        collect=dict(
            collector=dict(
                deepcopy_obs=False,
                transform_obs=False,
                collect_print_freq=100,
                cfg_type='SampleSerialCollectorDict',
                type='sample',
            ),
        ),
        eval=dict(
            evaluator=dict(
                eval_freq=1000,
                render={'render_freq': -1, 'mode': 'train_iter'},
                figure_path=None,
                cfg_type='InteractionSerialEvaluatorDict',
                stop_value=1000000,
                n_episode=1,
            ),
        ),
        other=dict(
            replay_buffer=dict(
                type='advanced',
                replay_buffer_size=4096,
                max_use=float('inf'),
                max_staleness=float('inf'),
                alpha=0.6,
                beta=0.4,
                anneal_step=100000,
                enable_track_used_data=False,
                deepcopy=False,
                thruput_controller=dict(
                    push_sample_rate_limit=dict(
                        max=float('inf'),
                        min=0,
                    ),
                    window_seconds=30,
                    sample_min_limit_ratio=1,
                ),
                monitor=dict(
                    sampled_data_attr=dict(
                        average_range=5,
                        print_freq=200,
                    ),
                    periodic_thruput=dict(
                        seconds=60,
                    ),
                ),
                cfg_type='AdvancedReplayBufferDict',
            ),
        ),
        on_policy=False,
        cuda=True,
        multi_gpu=False,
        bp_update_sync=True,
        traj_len_inf=False,
        use_wandb=False,
        use_rnd_model=False,
        sampled_algo=False,
        gumbel_algo=False,
        mcts_ctree=True,
        collector_env_num=1,
        evaluator_env_num=1,
        env_type='not_board_games',
        action_type='fixed_action_space',
        battle_mode='play_with_bot_mode',
        monitor_extra_statistics=True,
        game_segment_length=20,
        eval_offline=False,
        cal_dormant_ratio=False,
        analysis_sim_norm=False,
        analysis_dormant_ratio=False,
        transform2string=False,
        gray_scale=False,
        use_augmentation=False,
        augmentation=['shift', 'intensity'],
        ignore_done=False,
        update_per_collect=5,
        replay_ratio=0.25,
        batch_size=8,
        optim_type='Adam',
        learning_rate=0.01,
        target_update_freq=100,
        target_update_freq_for_intrinsic_reward=1000,
        weight_decay=0.0001,
        momentum=0.9,
        grad_clip_value=10,
        n_episode=1,
        num_segments=8,
        num_simulations=5,
        discount_factor=0.999,
        td_steps=3,
        num_unroll_steps=5,
        reward_loss_weight=1,
        value_loss_weight=0.25,
        policy_loss_weight=1,
        policy_entropy_weight=0,
        ssl_loss_weight=0,
        piecewise_decay_lr_scheduler=False,
        threshold_training_steps_for_final_lr=50000,
        manual_temperature_decay=True,
        threshold_training_steps_for_final_temperature=100000,
        fixed_temperature_value=0.25,
        use_ture_chance_label_in_chance_encoder=True,
        reanalyze_noise=True,
        reuse_search=False,
        collect_with_pure_policy=False,
        use_priority=True,
        priority_prob_alpha=0.6,
        priority_prob_beta=0.4,
        root_dirichlet_alpha=0.3,
        root_noise_weight=0.25,
        random_collect_episode_num=0,
        eps={'eps_greedy_exploration_in_collect': False, 'type': 'linear', 'start': 1.0, 'end': 0.05, 'decay': 100000},
        cfg_type='StochasticMuZeroPolicyDict',
        analyze_chance_distribution=False,
        afterstate_policy_loss_weight=1,
        afterstate_value_loss_weight=0.25,
        commitment_loss_weight=1.0,
        use_max_priority_for_new_data=True,
        import_names=['lzero.policy.stochastic_muzero'],
        model_path=None,
        reanalyze_ratio=0.0,
        stochastic_loss_weight=1.0,
        eval_freq=500,
        replay_buffer_size=1000,
        device='cuda',
    ),
)
main_config = EasyDict(main_config)
main_config = main_config
create_config = dict(
    env=dict(
        type='game_2048',
        import_names=['zoo.game_2048.envs.game_2048_env'],
    ),
    env_manager=dict(
        cfg_type='SyncSubprocessEnvManagerDict',
        type='subprocess',
    ),
    policy=dict(type='stochastic_muzero'),
)
create_config = EasyDict(create_config)
create_config = create_config
