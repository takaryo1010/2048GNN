exp_config = {
    'env': {
        'manager': {
            'episode_num': float("inf"),
            'max_retry': 1,
            'step_timeout': None,
            'auto_reset': True,
            'reset_timeout': None,
            'retry_type': 'reset',
            'retry_waiting_time': 0.1,
            'shared_memory': False,
            'copy_on_get': True,
            'context': 'fork',
            'wait_num': float("inf"),
            'step_wait_timeout': None,
            'connect_timeout': 60,
            'reset_inplace': False,
            'cfg_type': 'SyncSubprocessEnvManagerDict',
            'type': 'subprocess'
        },
        'stop_value': 1000000,
        'n_evaluator_episode': 1,
        'env_id': 'game_2048',
        'render_mode': None,
        'replay_format': 'gif',
        'replay_name_suffix': 'eval',
        'replay_path': None,
        'act_scale': True,
        'channel_last': False,
        'obs_type': 'dict_encoded_board',
        'reward_normalize': False,
        'reward_norm_scale': 100,
        'reward_type': 'raw',
        'max_tile': 65536,
        'delay_reward_step': 0,
        'prob_random_agent': 0.0,
        'max_episode_steps': 1000000,
        'is_collect': True,
        'ignore_legal_actions': True,
        'need_flatten': False,
        'num_of_possible_chance_tile': 2,
        'possible_tiles': array([2, 4]),
        'tile_probabilities': array([0.9, 0.1]),
        'cfg_type': 'Game2048EnvDict',
        'type': 'game_2048',
        'import_names': ['zoo.game_2048.envs.game_2048_env'],
        'obs_shape': (16, 4, 4),
        'collector_env_num': 8,
        'evaluator_env_num': 1
    },
    'policy': {
        'model': {
            'model_type': 'conv',
            'continuous_action_space': False,
            'observation_shape': (16, 4, 4),
            'self_supervised_learning_loss': True,
            'categorical_distribution': True,
            'image_channel': 16,
            'frame_stack_num': 1,
            'num_res_blocks': 1,
            'num_channels': 64,
            'support_scale': 300,
            'bias': True,
            'discrete_action_encoding_type': 'one_hot',
            'res_connection_in_dynamics': True,
            'norm_type': 'BN',
            'analysis_sim_norm': False,
            'analysis_dormant_ratio': False,
            'harmony_balance': False,
            'chance_space_size': 32,
            'type': 'GATStochasticMuZeroModel',
            'model': 'gat_stochastic',
            'action_space_size': 4,
            'num_heads': 4,
            'hidden_channels': 64,
            'num_gat_layers': 3,
            'state_dim': 256,
            'dropout': 0.1,
            'chance_encoder_num_layers': 2,
            'afterstate_reward_layers': 2,
            'value_head_channels': 16,
            'policy_head_channels': 16,
            'value_head_hidden_channels': [32],
            'policy_head_hidden_channels': [32],
            'reward_support_size': 601,
            'value_support_size': 601,
            'last_linear_layer_init_zero': True,
            'state_norm': False,
            'flatten_input_size_for_value_head': 256,
            'flatten_input_size_for_policy_head': 256
        },
        'learn': {
            'learner': {
                'train_iterations': 1000000000,
                'dataloader': {
                    'num_workers': 0
                },
                'log_policy': True,
                'hook': {
                    'load_ckpt_before_run': '',
                    'log_show_after_iter': 100,
                    'save_ckpt_after_iter': 10000,
                    'save_ckpt_after_run': True
                },
                'cfg_type': 'BaseLearnerDict'
            },
            'resume_training': False
        },
        'collect': {
            'collector': {
                'deepcopy_obs': False,
                'transform_obs': False,
                'collect_print_freq': 100,
                'cfg_type': 'SampleSerialCollectorDict',
                'type': 'sample'
            }
        },
        'eval': {
            'evaluator': {
                'eval_freq': 1000,
                'render': {
                    'render_freq': -1,
                    'mode': 'train_iter'
                },
                'figure_path': None,
                'cfg_type': 'InteractionSerialEvaluatorDict',
                'stop_value': 1000000,
                'n_episode': 1
            }
        },
        'other': {
            'replay_buffer': {
                'type': 'advanced',
                'replay_buffer_size': 4096,
                'max_use': float("inf"),
                'max_staleness': float("inf"),
                'alpha': 0.6,
                'beta': 0.4,
                'anneal_step': 100000,
                'enable_track_used_data': False,
                'deepcopy': False,
                'thruput_controller': {
                    'push_sample_rate_limit': {
                        'max': float("inf"),
                        'min': 0
                    },
                    'window_seconds': 30,
                    'sample_min_limit_ratio': 1
                },
                'monitor': {
                    'sampled_data_attr': {
                        'average_range': 5,
                        'print_freq': 200
                    },
                    'periodic_thruput': {
                        'seconds': 60
                    }
                },
                'cfg_type': 'AdvancedReplayBufferDict'
            },
            'commander': {
                'cfg_type': 'BaseSerialCommanderDict'
            }
        },
        'on_policy': False,
        'cuda': True,
        'multi_gpu': False,
        'bp_update_sync': True,
        'traj_len_inf': False,
        'use_wandb': False,
        'use_rnd_model': False,
        'sampled_algo': False,
        'gumbel_algo': False,
        'mcts_ctree': True,
        'collector_env_num': 8,
        'evaluator_env_num': 1,
        'env_type': 'not_board_games',
        'action_type': 'fixed_action_space',
        'battle_mode': 'play_with_bot_mode',
        'monitor_extra_statistics': True,
        'game_segment_length': 200,
        'eval_offline': False,
        'cal_dormant_ratio': False,
        'analysis_sim_norm': False,
        'analysis_dormant_ratio': False,
        'transform2string': False,
        'gray_scale': False,
        'use_augmentation': False,
        'augmentation': ['shift', 'intensity'],
        'ignore_done': False,
        'update_per_collect': 200,
        'replay_ratio': 0.25,
        'batch_size': 512,
        'optim_type': 'Adam',
        'learning_rate': 0.001,
        'target_update_freq': 100,
        'target_update_freq_for_intrinsic_reward': 1000,
        'weight_decay': 0.0001,
        'momentum': 0.9,
        'grad_clip_value': 10,
        'n_episode': 8,
        'num_segments': 8,
        'num_simulations': 100,
        'discount_factor': 0.999,
        'td_steps': 10,
        'num_unroll_steps': 5,
        'reward_loss_weight': 1,
        'value_loss_weight': 0.25,
        'policy_loss_weight': 1,
        'policy_entropy_weight': 0,
        'ssl_loss_weight': 0,
        'piecewise_decay_lr_scheduler': False,
        'threshold_training_steps_for_final_lr': 50000,
        'manual_temperature_decay': True,
        'threshold_training_steps_for_final_temperature': 100000,
        'fixed_temperature_value': 0.25,
        'use_ture_chance_label_in_chance_encoder': True,
        'reanalyze_noise': True,
        'reuse_search': False,
        'collect_with_pure_policy': False,
        'use_priority': True,
        'priority_prob_alpha': 0.6,
        'priority_prob_beta': 0.4,
        'root_dirichlet_alpha': 0.3,
        'root_noise_weight': 0.25,
        'random_collect_episode_num': 0,
        'eps': {
            'eps_greedy_exploration_in_collect': False,
            'type': 'linear',
            'start': 1.0,
            'end': 0.05,
            'decay': 100000
        },
        'cfg_type': 'StochasticMuZeroPolicyDict',
        'analyze_chance_distribution': False,
        'afterstate_policy_loss_weight': 1,
        'afterstate_value_loss_weight': 0.25,
        'commitment_loss_weight': 1.0,
        'use_max_priority_for_new_data': True,
        'type': 'stochastic_muzero',
        'import_names': ['lzero.policy.stochastic_muzero'],
        'model_path': None,
        'reanalyze_ratio': 0.0,
        'stochastic_loss_weight': 1.0,
        'eval_freq': 50000,
        'replay_buffer_size': 1000000,
        'device': 'cuda'
    },
    'exp_name':
    'data_gat_stochastic_4x4_transfer/game_2048_grid4_gat_stochastic_transfer_ns100_upc200_rer0.0_bs512_heads4_seed0',
    'seed': 0
}
