[2025-07-24 04:42:20][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 14.0, current episode: 1
[2025-07-24 04:42:22][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 40.0, current episode: 2
[2025-07-24 04:42:23][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 61.0, current episode: 3
[2025-07-24 04:42:23][muzero_evaluator.py:471][INFO] 
+-------+------------+---------------------+---------------+---------------+
| Name  | train_iter | ckpt_name           | episode_count | envstep_count |
+-------+------------+---------------------+---------------+---------------+
| Value | 0.000000   | iteration_0.pth.tar | 3.000000      | 115.000000    |
+-------+------------+---------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 38.333333               | 3.776000      | 30.455512           | 0.794492             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 38.333333   | 19.223828  | 61.000000  | 14.000000  |
+-------+-------------+------------+------------+------------+
+-------+---------------------+--------------------------+
| Name  | eval_episode_return | eval_episode_return_mean |
+-------+---------------------+--------------------------+
| Value | [14.0, 40.0, 61.0]  | 38.333333                |
+-------+---------------------+--------------------------+

[2025-07-24 04:42:39][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 10.0, current episode: 1
[2025-07-24 04:42:39][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 11.0, current episode: 2
[2025-07-24 04:42:39][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 14.0, current episode: 3
[2025-07-24 04:42:39][muzero_evaluator.py:471][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 100.000000 | iteration_100.pth.tar | 3.000000      | 35.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 11.666667               | 0.843708      | 41.483531           | 3.555731             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 11.666667   | 1.699673   | 14.000000  | 10.000000  |
+-------+-------------+------------+------------+------------+
+-------+---------------------+--------------------------+
| Name  | eval_episode_return | eval_episode_return_mean |
+-------+---------------------+--------------------------+
| Value | [14.0, 11.0, 10.0]  | 11.666667                |
+-------+---------------------+--------------------------+

[2025-07-24 04:42:59][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 167.0, current episode: 1
[2025-07-24 04:43:00][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 177.0, current episode: 2
[2025-07-24 04:43:00][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 180.0, current episode: 3
[2025-07-24 04:43:00][muzero_evaluator.py:471][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 200.000000 | iteration_200.pth.tar | 3.000000      | 524.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 174.666667              | 10.991298     | 47.674079           | 0.272943             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 174.666667  | 5.557777   | 180.000000 | 167.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [177.0, 180.0, 167.0] | 174.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:43:26][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 131.0, current episode: 1
[2025-07-24 04:43:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 155.0, current episode: 2
[2025-07-24 04:43:29][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 175.0, current episode: 3
[2025-07-24 04:43:29][muzero_evaluator.py:471][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 300.000000 | iteration_300.pth.tar | 3.000000      | 461.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 153.666667              | 11.097581     | 41.540584           | 0.270329             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 153.666667  | 17.987650  | 175.000000 | 131.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [175.0, 131.0, 155.0] | 153.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:43:58][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 122.0, current episode: 1
[2025-07-24 04:43:59][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 150.0, current episode: 2
[2025-07-24 04:44:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 186.0, current episode: 3
[2025-07-24 04:44:01][muzero_evaluator.py:471][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 400.000000 | iteration_400.pth.tar | 3.000000      | 458.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 152.666667              | 10.941698     | 41.858219           | 0.274180             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 152.666667  | 26.195844  | 186.000000 | 122.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [150.0, 122.0, 186.0] | 152.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:44:29][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 104.0, current episode: 1
[2025-07-24 04:44:29][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 106.0, current episode: 2
[2025-07-24 04:44:29][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 109.0, current episode: 3
[2025-07-24 04:44:29][muzero_evaluator.py:471][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 500.000000 | iteration_500.pth.tar | 3.000000      | 319.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 106.333333              | 6.368437      | 50.090787           | 0.471073             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 106.333333  | 2.054805   | 109.000000 | 104.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [104.0, 106.0, 109.0] | 106.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:44:53][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 68.0, current episode: 1
[2025-07-24 04:44:54][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 76.0, current episode: 2
[2025-07-24 04:44:54][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 77.0, current episode: 3
[2025-07-24 04:44:54][muzero_evaluator.py:471][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 600.000000 | iteration_600.pth.tar | 3.000000      | 221.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 73.666667               | 4.609511      | 47.944346           | 0.650828             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 73.666667   | 4.027682   | 77.000000  | 68.000000  |
+-------+-------------+------------+------------+------------+
+-------+---------------------+--------------------------+
| Name  | eval_episode_return | eval_episode_return_mean |
+-------+---------------------+--------------------------+
| Value | [77.0, 68.0, 76.0]  | 73.666667                |
+-------+---------------------+--------------------------+

[2025-07-24 04:45:12][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 107.0, current episode: 1
[2025-07-24 04:45:13][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 115.0, current episode: 2
[2025-07-24 04:45:13][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 118.0, current episode: 3
[2025-07-24 04:45:13][muzero_evaluator.py:471][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 700.000000 | iteration_700.pth.tar | 3.000000      | 340.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 113.333333              | 6.358262      | 53.473731           | 0.471827             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 113.333333  | 4.642796   | 118.000000 | 107.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [115.0, 118.0, 107.0] | 113.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:45:39][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 108.0, current episode: 1
[2025-07-24 04:45:39][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 108.0, current episode: 2
[2025-07-24 04:45:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 121.0, current episode: 3
[2025-07-24 04:45:40][muzero_evaluator.py:471][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 800.000000 | iteration_800.pth.tar | 3.000000      | 337.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 112.333333              | 7.342737      | 45.895694           | 0.408567             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 112.333333  | 6.128259   | 121.000000 | 108.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [121.0, 108.0, 108.0] | 112.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:46:07][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 123.0, current episode: 1
[2025-07-24 04:46:08][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 140.0, current episode: 2
[2025-07-24 04:46:09][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 147.0, current episode: 3
[2025-07-24 04:46:09][muzero_evaluator.py:471][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 900.000000 | iteration_900.pth.tar | 3.000000      | 410.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 136.666667              | 9.407799      | 43.580864           | 0.318884             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 136.666667  | 10.077478  | 147.000000 | 123.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [140.0, 147.0, 123.0] | 136.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:46:35][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 116.0, current episode: 1
[2025-07-24 04:46:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 133.0, current episode: 2
[2025-07-24 04:46:37][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 144.0, current episode: 3
[2025-07-24 04:46:37][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1000.000000 | iteration_1000.pth.tar | 3.000000      | 393.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 131.000000              | 8.107938      | 48.471014           | 0.370008             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 131.000000  | 11.518102  | 144.000000 | 116.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [116.0, 144.0, 133.0] | 131.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:47:04][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 110.0, current episode: 1
[2025-07-24 04:47:04][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 113.0, current episode: 2
[2025-07-24 04:47:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 124.0, current episode: 3
[2025-07-24 04:47:05][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1100.000000 | iteration_1100.pth.tar | 3.000000      | 347.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 115.666667              | 7.380877      | 47.013384           | 0.406456             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 115.666667  | 6.018490   | 124.000000 | 110.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [110.0, 113.0, 124.0] | 115.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:47:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 111.0, current episode: 1
[2025-07-24 04:47:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 125.0, current episode: 2
[2025-07-24 04:47:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 128.0, current episode: 3
[2025-07-24 04:47:31][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1200.000000 | iteration_1200.pth.tar | 3.000000      | 364.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 121.333333              | 7.164864      | 50.803475           | 0.418710             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 121.333333  | 7.408704   | 128.000000 | 111.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [125.0, 128.0, 111.0] | 121.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:47:56][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 138.0, current episode: 1
[2025-07-24 04:47:56][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 145.0, current episode: 2
[2025-07-24 04:47:56][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 154.0, current episode: 3
[2025-07-24 04:47:56][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1300.000000 | iteration_1300.pth.tar | 3.000000      | 437.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 145.666667              | 9.676838      | 45.159380           | 0.310019             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 145.666667  | 6.548961   | 154.000000 | 138.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [145.0, 154.0, 138.0] | 145.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:48:25][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 133.0, current episode: 1
[2025-07-24 04:48:26][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 139.0, current episode: 2
[2025-07-24 04:48:26][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 140.0, current episode: 3
[2025-07-24 04:48:26][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1400.000000 | iteration_1400.pth.tar | 3.000000      | 412.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 137.333333              | 9.103494      | 45.257348           | 0.329544             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 137.333333  | 3.091206   | 140.000000 | 133.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [140.0, 139.0, 133.0] | 137.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:48:54][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 135.0, current episode: 1
[2025-07-24 04:48:54][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 136.0, current episode: 2
[2025-07-24 04:48:54][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 146.0, current episode: 3
[2025-07-24 04:48:54][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1500.000000 | iteration_1500.pth.tar | 3.000000      | 417.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 139.000000              | 9.002505      | 46.320441           | 0.333241             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 139.000000  | 4.966555   | 146.000000 | 135.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [136.0, 146.0, 135.0] | 139.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:49:22][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 108.0, current episode: 1
[2025-07-24 04:49:22][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 115.0, current episode: 2
[2025-07-24 04:49:22][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 117.0, current episode: 3
[2025-07-24 04:49:22][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1600.000000 | iteration_1600.pth.tar | 3.000000      | 340.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 113.333333              | 7.577560      | 44.869327           | 0.395906             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 113.333333  | 3.858612   | 117.000000 | 108.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [115.0, 108.0, 117.0] | 113.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:49:52][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 112.0, current episode: 1
[2025-07-24 04:49:52][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 115.0, current episode: 2
[2025-07-24 04:49:52][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 122.0, current episode: 3
[2025-07-24 04:49:52][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1700.000000 | iteration_1700.pth.tar | 3.000000      | 349.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 116.333333              | 7.913193      | 44.103563           | 0.379114             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 116.333333  | 4.189935   | 122.000000 | 112.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [112.0, 115.0, 122.0] | 116.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:50:21][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 110.0, current episode: 1
[2025-07-24 04:50:22][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 118.0, current episode: 2
[2025-07-24 04:50:22][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 127.0, current episode: 3
[2025-07-24 04:50:22][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1800.000000 | iteration_1800.pth.tar | 3.000000      | 355.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 118.333333              | 7.566963      | 46.914463           | 0.396460             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 118.333333  | 6.944222   | 127.000000 | 110.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [110.0, 118.0, 127.0] | 118.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:50:49][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 116.0, current episode: 1
[2025-07-24 04:50:50][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 127.0, current episode: 2
[2025-07-24 04:50:50][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 134.0, current episode: 3
[2025-07-24 04:50:50][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 1900.000000 | iteration_1900.pth.tar | 3.000000      | 377.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 125.666667              | 7.812506      | 48.255964           | 0.384000             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 125.666667  | 7.408704   | 134.000000 | 116.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [134.0, 127.0, 116.0] | 125.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:51:15][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 120.0, current episode: 1
[2025-07-24 04:51:17][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 129.0, current episode: 2
[2025-07-24 04:51:17][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 130.0, current episode: 3
[2025-07-24 04:51:17][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2000.000000 | iteration_2000.pth.tar | 3.000000      | 379.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 126.333333              | 8.618717      | 43.974064           | 0.348080             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 126.333333  | 4.496913   | 130.000000 | 120.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [130.0, 129.0, 120.0] | 126.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:51:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 122.0, current episode: 1
[2025-07-24 04:51:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 122.0, current episode: 2
[2025-07-24 04:51:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 125.0, current episode: 3
[2025-07-24 04:51:40][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2100.000000 | iteration_2100.pth.tar | 3.000000      | 369.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 123.000000              | 7.596000      | 48.578202           | 0.394945             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 123.000000  | 1.414214   | 125.000000 | 122.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [122.0, 125.0, 122.0] | 123.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:52:06][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 125.0, current episode: 1
[2025-07-24 04:52:06][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 127.0, current episode: 2
[2025-07-24 04:52:06][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 133.0, current episode: 3
[2025-07-24 04:52:06][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2200.000000 | iteration_2200.pth.tar | 3.000000      | 385.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 128.333333              | 7.551991      | 50.979932           | 0.397246             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 128.333333  | 3.399346   | 133.000000 | 125.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [127.0, 125.0, 133.0] | 128.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:52:34][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 108.0, current episode: 1
[2025-07-24 04:52:34][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 119.0, current episode: 2
[2025-07-24 04:52:35][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 121.0, current episode: 3
[2025-07-24 04:52:35][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2300.000000 | iteration_2300.pth.tar | 3.000000      | 348.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 116.000000              | 7.717067      | 45.094851           | 0.388749             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 116.000000  | 5.715476   | 121.000000 | 108.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [108.0, 119.0, 121.0] | 116.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:53:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 121.0, current episode: 1
[2025-07-24 04:53:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 125.0, current episode: 2
[2025-07-24 04:53:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 128.0, current episode: 3
[2025-07-24 04:53:01][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2400.000000 | iteration_2400.pth.tar | 3.000000      | 374.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 124.666667              | 7.786941      | 48.029127           | 0.385260             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 124.666667  | 2.867442   | 128.000000 | 121.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [125.0, 128.0, 121.0] | 124.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:53:24][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 121.0, current episode: 1
[2025-07-24 04:53:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 129.0, current episode: 2
[2025-07-24 04:53:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 139.0, current episode: 3
[2025-07-24 04:53:28][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2500.000000 | iteration_2500.pth.tar | 3.000000      | 389.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 129.666667              | 8.506898      | 45.727594           | 0.352655             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 129.666667  | 7.363574   | 139.000000 | 121.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [121.0, 129.0, 139.0] | 129.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:53:53][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 106.0, current episode: 1
[2025-07-24 04:53:53][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 109.0, current episode: 2
[2025-07-24 04:53:54][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 112.0, current episode: 3
[2025-07-24 04:53:54][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2600.000000 | iteration_2600.pth.tar | 3.000000      | 327.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 109.000000              | 7.440285      | 43.949931           | 0.403210             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 109.000000  | 2.449490   | 112.000000 | 106.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [109.0, 106.0, 112.0] | 109.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:54:20][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 121.0, current episode: 1
[2025-07-24 04:54:20][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 124.0, current episode: 2
[2025-07-24 04:54:20][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 129.0, current episode: 3
[2025-07-24 04:54:20][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2700.000000 | iteration_2700.pth.tar | 3.000000      | 374.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 124.666667              | 8.031909      | 46.564274           | 0.373510             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 124.666667  | 3.299832   | 129.000000 | 121.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [124.0, 121.0, 129.0] | 124.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:54:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 118.0, current episode: 1
[2025-07-24 04:54:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 123.0, current episode: 2
[2025-07-24 04:54:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 126.0, current episode: 3
[2025-07-24 04:54:45][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2800.000000 | iteration_2800.pth.tar | 3.000000      | 367.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 122.333333              | 6.907628      | 53.129671           | 0.434302             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 122.333333  | 3.299832   | 126.000000 | 118.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [123.0, 118.0, 126.0] | 122.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:55:12][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 123.0, current episode: 1
[2025-07-24 04:55:12][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 125.0, current episode: 2
[2025-07-24 04:55:12][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 125.0, current episode: 3
[2025-07-24 04:55:12][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2900.000000 | iteration_2900.pth.tar | 3.000000      | 373.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 124.333333              | 7.688448      | 48.514344           | 0.390196             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 124.333333  | 0.942809   | 125.000000 | 123.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [123.0, 125.0, 125.0] | 124.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:55:34][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 122.0, current episode: 1
[2025-07-24 04:55:34][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 125.0, current episode: 2
[2025-07-24 04:55:35][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 129.0, current episode: 3
[2025-07-24 04:55:35][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3000.000000 | iteration_3000.pth.tar | 3.000000      | 376.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 125.333333              | 7.397389      | 50.828744           | 0.405548             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 125.333333  | 2.867442   | 129.000000 | 122.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [125.0, 122.0, 129.0] | 125.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:55:59][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 114.0, current episode: 1
[2025-07-24 04:56:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 135.0, current episode: 2
[2025-07-24 04:56:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 138.0, current episode: 3
[2025-07-24 04:56:01][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3100.000000 | iteration_3100.pth.tar | 3.000000      | 387.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 129.000000              | 8.291146      | 46.676301           | 0.361832             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 129.000000  | 10.677078  | 138.000000 | 114.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [138.0, 135.0, 114.0] | 129.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:56:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 139.0, current episode: 1
[2025-07-24 04:56:29][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 147.0, current episode: 2
[2025-07-24 04:56:29][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 148.0, current episode: 3
[2025-07-24 04:56:29][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3200.000000 | iteration_3200.pth.tar | 3.000000      | 434.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 144.666667              | 8.775197      | 49.457578           | 0.341873             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 144.666667  | 4.027682   | 148.000000 | 139.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [147.0, 148.0, 139.0] | 144.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:56:56][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 133.0, current episode: 1
[2025-07-24 04:56:56][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 138.0, current episode: 2
[2025-07-24 04:56:56][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 147.0, current episode: 3
[2025-07-24 04:56:56][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3300.000000 | iteration_3300.pth.tar | 3.000000      | 418.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 139.333333              | 8.291440      | 50.413436           | 0.361819             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 139.333333  | 5.792716   | 147.000000 | 133.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [138.0, 147.0, 133.0] | 139.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:57:26][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 129.0, current episode: 1
[2025-07-24 04:57:26][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 133.0, current episode: 2
[2025-07-24 04:57:26][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 136.0, current episode: 3
[2025-07-24 04:57:26][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3400.000000 | iteration_3400.pth.tar | 3.000000      | 398.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 132.666667              | 9.723313      | 40.932553           | 0.308537             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 132.666667  | 2.867442   | 136.000000 | 129.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [136.0, 133.0, 129.0] | 132.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:57:54][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 145.0, current episode: 1
[2025-07-24 04:57:54][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 145.0, current episode: 2
[2025-07-24 04:57:55][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 148.0, current episode: 3
[2025-07-24 04:57:55][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3500.000000 | iteration_3500.pth.tar | 3.000000      | 438.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 146.000000              | 7.559913      | 57.937174           | 0.396830             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 146.000000  | 1.414214   | 148.000000 | 145.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [145.0, 145.0, 148.0] | 146.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:58:18][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 126.0, current episode: 1
[2025-07-24 04:58:18][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 130.0, current episode: 2
[2025-07-24 04:58:19][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 148.0, current episode: 3
[2025-07-24 04:58:19][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3600.000000 | iteration_3600.pth.tar | 3.000000      | 404.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 134.666667              | 8.597541      | 46.990180           | 0.348937             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 134.666667  | 9.568467   | 148.000000 | 126.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [130.0, 148.0, 126.0] | 134.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:58:46][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 130.0, current episode: 1
[2025-07-24 04:58:46][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 132.0, current episode: 2
[2025-07-24 04:58:46][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 134.0, current episode: 3
[2025-07-24 04:58:46][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3700.000000 | iteration_3700.pth.tar | 3.000000      | 396.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 132.000000              | 7.897683      | 50.141288           | 0.379858             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 132.000000  | 1.632993   | 134.000000 | 130.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [130.0, 132.0, 134.0] | 132.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:59:16][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 152.0, current episode: 1
[2025-07-24 04:59:16][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 153.0, current episode: 2
[2025-07-24 04:59:16][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 158.0, current episode: 3
[2025-07-24 04:59:16][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3800.000000 | iteration_3800.pth.tar | 3.000000      | 463.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 154.333333              | 10.134462     | 45.685701           | 0.296020             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 154.333333  | 2.624669   | 158.000000 | 152.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [153.0, 158.0, 152.0] | 154.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 04:59:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 161.0, current episode: 1
[2025-07-24 04:59:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 162.0, current episode: 2
[2025-07-24 04:59:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 162.0, current episode: 3
[2025-07-24 04:59:45][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 3900.000000 | iteration_3900.pth.tar | 3.000000      | 485.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 161.666667              | 9.672385      | 50.142753           | 0.310161             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 161.666667  | 0.471405   | 162.000000 | 161.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [161.0, 162.0, 162.0] | 161.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:00:15][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 133.0, current episode: 1
[2025-07-24 05:00:16][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 142.0, current episode: 2
[2025-07-24 05:00:17][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 164.0, current episode: 3
[2025-07-24 05:00:17][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4000.000000 | iteration_4000.pth.tar | 3.000000      | 439.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 146.333333              | 10.068721     | 43.600375           | 0.297952             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 146.333333  | 13.021350  | 164.000000 | 133.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [133.0, 164.0, 142.0] | 146.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:00:44][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 139.0, current episode: 1
[2025-07-24 05:00:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 142.0, current episode: 2
[2025-07-24 05:00:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 144.0, current episode: 3
[2025-07-24 05:00:45][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4100.000000 | iteration_4100.pth.tar | 3.000000      | 425.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 141.666667              | 8.199107      | 51.834911           | 0.365893             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 141.666667  | 2.054805   | 144.000000 | 139.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [139.0, 142.0, 144.0] | 141.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:01:12][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 143.0, current episode: 1
[2025-07-24 05:01:12][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 146.0, current episode: 2
[2025-07-24 05:01:13][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 161.0, current episode: 3
[2025-07-24 05:01:13][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4200.000000 | iteration_4200.pth.tar | 3.000000      | 450.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 150.000000              | 9.362321      | 48.065003           | 0.320433             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 150.000000  | 7.874008   | 161.000000 | 143.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [161.0, 143.0, 146.0] | 150.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:01:38][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 145.0, current episode: 1
[2025-07-24 05:01:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 156.0, current episode: 2
[2025-07-24 05:01:41][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 158.0, current episode: 3
[2025-07-24 05:01:41][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4300.000000 | iteration_4300.pth.tar | 3.000000      | 459.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 153.000000              | 8.782337      | 52.263994           | 0.341595             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 153.000000  | 5.715476   | 158.000000 | 145.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [145.0, 156.0, 158.0] | 153.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:02:04][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 156.0, current episode: 1
[2025-07-24 05:02:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 160.0, current episode: 2
[2025-07-24 05:02:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 163.0, current episode: 3
[2025-07-24 05:02:05][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4400.000000 | iteration_4400.pth.tar | 3.000000      | 479.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 159.666667              | 8.779896      | 54.556452           | 0.341690             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 159.666667  | 2.867442   | 163.000000 | 156.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [156.0, 160.0, 163.0] | 159.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:02:32][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 148.0, current episode: 1
[2025-07-24 05:02:32][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 149.0, current episode: 2
[2025-07-24 05:02:33][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 161.0, current episode: 3
[2025-07-24 05:02:33][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4500.000000 | iteration_4500.pth.tar | 3.000000      | 458.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 152.666667              | 9.593059      | 47.742854           | 0.312726             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 152.666667  | 5.906682   | 161.000000 | 148.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [148.0, 149.0, 161.0] | 152.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:03:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 138.0, current episode: 1
[2025-07-24 05:03:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 138.0, current episode: 2
[2025-07-24 05:03:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 146.0, current episode: 3
[2025-07-24 05:03:01][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4600.000000 | iteration_4600.pth.tar | 3.000000      | 422.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 140.666667              | 8.070146      | 52.291492           | 0.371740             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 140.666667  | 3.771236   | 146.000000 | 138.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [138.0, 146.0, 138.0] | 140.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:03:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 141.0, current episode: 1
[2025-07-24 05:03:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 148.0, current episode: 2
[2025-07-24 05:03:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 153.0, current episode: 3
[2025-07-24 05:03:28][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4700.000000 | iteration_4700.pth.tar | 3.000000      | 442.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 147.333333              | 8.128127      | 54.379072           | 0.369089             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 147.333333  | 4.921608   | 153.000000 | 141.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [141.0, 148.0, 153.0] | 147.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:03:57][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 137.0, current episode: 1
[2025-07-24 05:03:58][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 154.0, current episode: 2
[2025-07-24 05:03:58][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 156.0, current episode: 3
[2025-07-24 05:03:58][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4800.000000 | iteration_4800.pth.tar | 3.000000      | 447.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 149.000000              | 9.970475      | 44.832369           | 0.300888             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 149.000000  | 8.524475   | 156.000000 | 137.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [156.0, 154.0, 137.0] | 149.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:04:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 154.0, current episode: 1
[2025-07-24 05:04:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 154.0, current episode: 2
[2025-07-24 05:04:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 155.0, current episode: 3
[2025-07-24 05:04:28][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4900.000000 | iteration_4900.pth.tar | 3.000000      | 463.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 154.333333              | 9.185635      | 50.404791           | 0.326597             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 154.333333  | 0.471405   | 155.000000 | 154.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [154.0, 155.0, 154.0] | 154.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:04:58][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 146.0, current episode: 1
[2025-07-24 05:04:59][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 160.0, current episode: 2
[2025-07-24 05:04:59][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 160.0, current episode: 3
[2025-07-24 05:04:59][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5000.000000 | iteration_5000.pth.tar | 3.000000      | 466.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 155.333333              | 10.046243     | 46.385499           | 0.298619             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 155.333333  | 6.599663   | 160.000000 | 146.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [160.0, 160.0, 146.0] | 155.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:05:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 160.0, current episode: 1
[2025-07-24 05:05:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 160.0, current episode: 2
[2025-07-24 05:05:32][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 173.0, current episode: 3
[2025-07-24 05:05:32][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5100.000000 | iteration_5100.pth.tar | 3.000000      | 493.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 164.333333              | 11.076720     | 44.507762           | 0.270838             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 164.333333  | 6.128259   | 173.000000 | 160.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [160.0, 160.0, 173.0] | 164.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:06:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 153.0, current episode: 1
[2025-07-24 05:06:03][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 154.0, current episode: 2
[2025-07-24 05:06:04][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 159.0, current episode: 3
[2025-07-24 05:06:04][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5200.000000 | iteration_5200.pth.tar | 3.000000      | 466.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 155.333333              | 10.440551     | 44.633661           | 0.287341             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 155.333333  | 2.624669   | 159.000000 | 153.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [154.0, 159.0, 153.0] | 155.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:06:30][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 159.0, current episode: 1
[2025-07-24 05:06:30][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 167.0, current episode: 2
[2025-07-24 05:06:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 171.0, current episode: 3
[2025-07-24 05:06:31][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5300.000000 | iteration_5300.pth.tar | 3.000000      | 497.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 165.666667              | 10.251968     | 48.478498           | 0.292627             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 165.666667  | 4.988877   | 171.000000 | 159.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [171.0, 167.0, 159.0] | 165.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:07:00][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 153.0, current episode: 1
[2025-07-24 05:07:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 160.0, current episode: 2
[2025-07-24 05:07:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 164.0, current episode: 3
[2025-07-24 05:07:01][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5400.000000 | iteration_5400.pth.tar | 3.000000      | 477.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 159.000000              | 9.825004      | 48.549599           | 0.305343             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 159.000000  | 4.546061   | 164.000000 | 153.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [164.0, 160.0, 153.0] | 159.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:07:30][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 157.0, current episode: 1
[2025-07-24 05:07:30][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 167.0, current episode: 2
[2025-07-24 05:07:30][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 168.0, current episode: 3
[2025-07-24 05:07:30][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5500.000000 | iteration_5500.pth.tar | 3.000000      | 492.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 164.000000              | 9.572816      | 51.395533           | 0.313387             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 164.000000  | 4.966555   | 168.000000 | 157.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [157.0, 168.0, 167.0] | 164.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:08:00][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 154.0, current episode: 1
[2025-07-24 05:08:00][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 157.0, current episode: 2
[2025-07-24 05:08:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 161.0, current episode: 3
[2025-07-24 05:08:01][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5600.000000 | iteration_5600.pth.tar | 3.000000      | 472.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 157.333333              | 9.364018      | 50.405715           | 0.320375             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 157.333333  | 2.867442   | 161.000000 | 154.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [157.0, 154.0, 161.0] | 157.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:08:30][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 160.0, current episode: 1
[2025-07-24 05:08:30][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 171.0, current episode: 2
[2025-07-24 05:08:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 174.0, current episode: 3
[2025-07-24 05:08:31][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5700.000000 | iteration_5700.pth.tar | 3.000000      | 505.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 168.333333              | 9.008790      | 56.056362           | 0.333008             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 168.333333  | 6.018490   | 174.000000 | 160.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [171.0, 160.0, 174.0] | 168.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:09:02][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 163.0, current episode: 1
[2025-07-24 05:09:02][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 172.0, current episode: 2
[2025-07-24 05:09:03][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 174.0, current episode: 3
[2025-07-24 05:09:03][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5800.000000 | iteration_5800.pth.tar | 3.000000      | 509.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 169.666667              | 10.946048     | 46.500802           | 0.274072             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 169.666667  | 4.784233   | 174.000000 | 163.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [174.0, 172.0, 163.0] | 169.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:09:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 168.0, current episode: 1
[2025-07-24 05:09:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 172.0, current episode: 2
[2025-07-24 05:09:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 185.0, current episode: 3
[2025-07-24 05:09:31][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 5900.000000 | iteration_5900.pth.tar | 3.000000      | 525.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 175.000000              | 9.699221      | 54.128060           | 0.309303             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 175.000000  | 7.257180   | 185.000000 | 168.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [172.0, 185.0, 168.0] | 175.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:10:02][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 160.0, current episode: 1
[2025-07-24 05:10:03][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 170.0, current episode: 2
[2025-07-24 05:10:03][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 175.0, current episode: 3
[2025-07-24 05:10:03][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6000.000000 | iteration_6000.pth.tar | 3.000000      | 505.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 168.333333              | 10.719587     | 47.110024           | 0.279862             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 168.333333  | 6.236096   | 175.000000 | 160.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [170.0, 175.0, 160.0] | 168.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:10:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 170.0, current episode: 1
[2025-07-24 05:10:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 172.0, current episode: 2
[2025-07-24 05:10:37][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 178.0, current episode: 3
[2025-07-24 05:10:37][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6100.000000 | iteration_6100.pth.tar | 3.000000      | 520.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 173.333333              | 11.550833     | 45.018398           | 0.259722             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 173.333333  | 3.399346   | 178.000000 | 170.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [170.0, 178.0, 172.0] | 173.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:11:08][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 170.0, current episode: 1
[2025-07-24 05:11:08][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 174.0, current episode: 2
[2025-07-24 05:11:09][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 176.0, current episode: 3
[2025-07-24 05:11:09][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6200.000000 | iteration_6200.pth.tar | 3.000000      | 520.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 173.333333              | 10.524474     | 49.408647           | 0.285050             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 173.333333  | 2.494438   | 176.000000 | 170.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [170.0, 176.0, 174.0] | 173.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:11:38][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 166.0, current episode: 1
[2025-07-24 05:11:38][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 173.0, current episode: 2
[2025-07-24 05:11:38][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 179.0, current episode: 3
[2025-07-24 05:11:38][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6300.000000 | iteration_6300.pth.tar | 3.000000      | 518.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 172.666667              | 8.906562      | 58.159363           | 0.336830             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 172.666667  | 5.312459   | 179.000000 | 166.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [166.0, 179.0, 173.0] | 172.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:12:08][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 163.0, current episode: 1
[2025-07-24 05:12:08][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 163.0, current episode: 2
[2025-07-24 05:12:09][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 183.0, current episode: 3
[2025-07-24 05:12:09][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6400.000000 | iteration_6400.pth.tar | 3.000000      | 509.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 169.666667              | 10.037956     | 50.707534           | 0.298866             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 169.666667  | 9.428090   | 183.000000 | 163.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [163.0, 183.0, 163.0] | 169.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:12:35][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 161.0, current episode: 1
[2025-07-24 05:12:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 164.0, current episode: 2
[2025-07-24 05:12:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 172.0, current episode: 3
[2025-07-24 05:12:36][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6500.000000 | iteration_6500.pth.tar | 3.000000      | 497.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 165.666667              | 9.346354      | 53.175813           | 0.320981             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 165.666667  | 4.642796   | 172.000000 | 161.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [172.0, 161.0, 164.0] | 165.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:13:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 173.0, current episode: 1
[2025-07-24 05:13:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 184.0, current episode: 2
[2025-07-24 05:13:06][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 195.0, current episode: 3
[2025-07-24 05:13:06][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6600.000000 | iteration_6600.pth.tar | 3.000000      | 552.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 184.000000              | 10.736483     | 51.413482           | 0.279421             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 184.000000  | 8.981462   | 195.000000 | 173.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [184.0, 195.0, 173.0] | 184.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:13:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 180.0, current episode: 1
[2025-07-24 05:13:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 181.0, current episode: 2
[2025-07-24 05:13:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 187.0, current episode: 3
[2025-07-24 05:13:36][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6700.000000 | iteration_6700.pth.tar | 3.000000      | 548.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 182.666667              | 10.588330     | 51.755092           | 0.283331             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 182.666667  | 3.091206   | 187.000000 | 180.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [181.0, 187.0, 180.0] | 182.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:14:07][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 185.0, current episode: 1
[2025-07-24 05:14:08][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 193.0, current episode: 2
[2025-07-24 05:14:08][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 194.0, current episode: 3
[2025-07-24 05:14:08][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6800.000000 | iteration_6800.pth.tar | 3.000000      | 572.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 190.666667              | 11.153290     | 51.285316           | 0.268979             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 190.666667  | 4.027682   | 194.000000 | 185.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [185.0, 193.0, 194.0] | 190.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:14:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 200.0, current episode: 1
[2025-07-24 05:14:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 200.0, current episode: 2
[2025-07-24 05:14:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 200.0, current episode: 3
[2025-07-24 05:14:40][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6900.000000 | iteration_6900.pth.tar | 3.000000      | 600.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 200.000000              | 11.226837     | 53.443370           | 0.267217             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 200.000000  | 0.000000   | 200.000000 | 200.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [200.0, 200.0, 200.0] | 200.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:14:40][muzero_evaluator.py:489][INFO] [LightZero serial pipeline] Current episode_return: 200.0 is greater than stop_value: 200, so your MCTS/RL agent is converged, you can refer to 'log/evaluator/evaluator_logger.txt' for details.
