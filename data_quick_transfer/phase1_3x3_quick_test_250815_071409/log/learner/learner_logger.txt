[2025-08-15 07:14:10][base_learner.py:360][INFO] [RANK0]: DI-engine DRL Policy
GATStochasticMuZeroModel(
  (representation_network): GATRepresentationNetwork(
    (input_proj): Linear(in_features=16, out_features=64, bias=True)
    (gat_layers): ModuleList(
      (0): GATConv(64, 64, heads=4)
      (1-2): 2 x GATConv(256, 64, heads=4)
    )
    (output_proj): Sequential(
      (0): Linear(in_features=64, out_features=128, bias=True)
      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=256, bias=True)
      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (activation): ReLU()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (afterstate_dynamics_network): GATAfterstateDynamicsNetwork(
    (action_embedding): Embedding(4, 64)
    (state_action_fusion): Sequential(
      (0): Linear(in_features=320, out_features=128, bias=True)
      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=256, bias=True)
      (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (8): ReLU()
    )
    (to_grid_proj): Linear(in_features=256, out_features=144, bias=True)
    (gat_afterstate): GATRepresentationNetwork(
      (input_proj): Linear(in_features=16, out_features=64, bias=True)
      (gat_layers): ModuleList(
        (0): GATConv(64, 64, heads=4)
        (1): GATConv(256, 64, heads=4)
      )
      (output_proj): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): ReLU()
        (3): Linear(in_features=128, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU()
      )
      (activation): ReLU()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (chance_encoder): GATChanceEncoder(
    (chance_embedding): Embedding(18, 64)
    (gat_chance): GATRepresentationNetwork(
      (input_proj): Linear(in_features=32, out_features=64, bias=True)
      (gat_layers): ModuleList(
        (0): GATConv(64, 64, heads=4)
        (1): GATConv(256, 64, heads=4)
      )
      (output_proj): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): ReLU()
        (3): Linear(in_features=128, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU()
      )
      (activation): ReLU()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (chance_fusion): Sequential(
      (0): Linear(in_features=320, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=256, out_features=256, bias=True)
      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
  )
  (state_to_grid_projection): Linear(in_features=256, out_features=144, bias=True)
  (reward_head_afterstate): Sequential(
    (0): Linear(in_features=256, out_features=32, bias=True)
    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (2): ReLU()
    (3): Linear(in_features=32, out_features=601, bias=True)
  )
  (value_head): Sequential(
    (0): Linear(in_features=256, out_features=32, bias=True)
    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (2): ReLU()
    (3): Linear(in_features=32, out_features=601, bias=True)
  )
  (policy_head): Sequential(
    (0): Linear(in_features=256, out_features=32, bias=True)
    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (2): ReLU()
    (3): Linear(in_features=32, out_features=4, bias=True)
  )
  (afterstate_policy_head): Sequential(
    (0): Linear(in_features=256, out_features=32, bias=True)
    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (2): ReLU()
    (3): Linear(in_features=32, out_features=18, bias=True)
  )
)
[2025-08-15 07:14:25][base_learner.py:360][INFO] [RANK0]: learner save ckpt in ./data_quick_transfer/phase1_3x3_quick_test_250815_071409/ckpt/ckpt_best.pth.tar
[2025-08-15 07:15:22][base_learner.py:360][INFO] [RANK0]: === Training Iteration 0 Result ===
[2025-08-15 07:15:22][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 60.971516               | 72.685074      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 8.111988        | 31.992973       | 38.391567      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 14.451859                  | 31.992973                 | 0.532118            | 78.433731          |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.093750          | 79.494690        | -0.000006             | -0.000006            |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.505021                      | 7.526626                     | 1.850232                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:15:22][base_learner.py:360][INFO] [RANK0]: learner save ckpt in ./data_quick_transfer/phase1_3x3_quick_test_250815_071409/ckpt/iteration_0.pth.tar
[2025-08-15 07:18:06][base_learner.py:360][INFO] [RANK0]: === Training Iteration 100 Result ===
[2025-08-15 07:18:06][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 18.648152               | 38.419389      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 8.019368        | 8.781736        | 15.742076      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.851423                  | 13.215527                 | 0.527462            | 53.363096          |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 9.492898          | 92.731194        | 5.304560              | 68.670514            |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.642615                      | 8.124787                     | 0.202711                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:20:49][base_learner.py:360][INFO] [RANK0]: === Training Iteration 200 Result ===
[2025-08-15 07:20:49][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 15.499788               | 38.525398      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.942106        | 8.877111        | 16.038899      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.787370                  | 13.534387                 | 0.525489            | 78.683499          |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 10.346118         | 149.638279       | 5.573254              | 119.892869           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.696338                      | 10.457072                    | 0.181747                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:23:21][base_learner.py:360][INFO] [RANK0]: === Training Iteration 300 Result ===
[2025-08-15 07:23:21][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 15.582757               | 37.976739      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.879038        | 8.817692        | 15.171340      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.748697                  | 12.854792                 | 0.524779            | 91.898630          |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 10.198390         | 190.502267       | 5.585304              | 156.493122           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.691298                      | 11.941303                    | 0.180983                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:25:54][base_learner.py:360][INFO] [RANK0]: === Training Iteration 400 Result ===
[2025-08-15 07:25:54][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 16.324089               | 37.732646      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.878300        | 8.713050        | 14.869764      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.730777                  | 12.671295                 | 0.525253            | 103.239977         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 9.804925          | 217.124119       | 5.664110              | 189.582574           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.650264                      | 12.685373                    | 0.196635                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:28:33][base_learner.py:360][INFO] [RANK0]: === Training Iteration 500 Result ===
[2025-08-15 07:28:33][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 16.275278               | 37.277814      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.849781        | 8.564773        | 14.213223      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.764482                  | 12.080249                 | 0.525410            | 111.401315         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 9.265625          | 245.223304       | 5.483236              | 211.830816           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.582331                      | 13.578730                    | 0.206548                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:30:53][base_learner.py:360][INFO] [RANK0]: === Training Iteration 600 Result ===
[2025-08-15 07:30:53][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 19.040478               | 36.979305      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.802084        | 8.408428        | 13.897264      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.806523                  | 11.852072                 | 0.524937            | 120.354399         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.719224          | 255.386849       | 5.390874              | 226.734062           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.530671                      | 13.767371                    | 0.233490                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:33:08][base_learner.py:360][INFO] [RANK0]: === Training Iteration 700 Result ===
[2025-08-15 07:33:08][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 18.573349               | 36.902795      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.859077        | 8.387773        | 13.623242      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.831105                  | 11.571632                 | 0.526121            | 120.637200         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.733902          | 273.745098       | 5.367875              | 244.770476           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.526453                      | 14.427340                    | 0.252024                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:35:21][base_learner.py:360][INFO] [RANK0]: === Training Iteration 800 Result ===
[2025-08-15 07:35:21][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 18.613743               | 36.776579      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.864005        | 8.355724        | 13.516787      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.772084                  | 11.525997                 | 0.524069            | 129.288216         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.463542          | 275.100677       | 5.345989              | 254.428206           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.492214                      | 14.326899                    | 0.249868                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:37:57][base_learner.py:360][INFO] [RANK0]: === Training Iteration 900 Result ===
[2025-08-15 07:37:57][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 21.099484               | 36.567472      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.796578        | 8.226765        | 13.430191      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.804790                  | 11.433096                 | 0.523516            | 128.045127         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.273201          | 270.194278       | 5.005205              | 246.662828           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.471419                      | 14.200366                    | 0.288053                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:40:33][base_learner.py:360][INFO] [RANK0]: === Training Iteration 1000 Result ===
[2025-08-15 07:40:33][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 22.773753               | 36.779825      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.808733        | 8.299476        | 13.795729      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.757179                  | 11.763535                 | 0.524621            | 127.036906         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.350852          | 266.271086       | 5.011314              | 235.740458           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.485782                      | 14.047047                    | 0.254385                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:43:17][base_learner.py:360][INFO] [RANK0]: === Training Iteration 1100 Result ===
[2025-08-15 07:43:17][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 22.696316               | 36.857498      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.846283        | 8.273891        | 13.944190      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.766982                  | 11.854479                 | 0.520676            | 127.793378         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.432765          | 266.785765       | 4.916127              | 236.833692           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.482997                      | 14.108913                    | 0.268593                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:45:57][base_learner.py:360][INFO] [RANK0]: === Training Iteration 1200 Result ===
[2025-08-15 07:45:57][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 22.450026               | 36.728970      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.889834        | 8.279128        | 13.573660      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.753511                  | 11.544058                 | 0.527068            | 125.989217         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.325758          | 269.508287       | 5.237390              | 233.917109           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.473708                      | 14.241053                    | 0.253895                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:48:07][base_learner.py:360][INFO] [RANK0]: === Training Iteration 1300 Result ===
[2025-08-15 07:48:07][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 20.882330               | 36.826050      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.860912        | 8.332686        | 13.680074      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.767851                  | 11.682367                 | 0.523990            | 136.331006         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.546875          | 269.185250       | 5.238234              | 223.357984           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.504950                      | 14.186405                    | 0.291740                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:50:52][base_learner.py:360][INFO] [RANK0]: === Training Iteration 1400 Result ===
[2025-08-15 07:50:52][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 19.793523               | 36.812378      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.869892        | 8.197741        | 13.829642      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.814580                  | 11.784012                 | 0.526752            | 125.764544         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.238163          | 266.055068       | 5.222463              | 234.058744           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.451012                      | 14.093645                    | 0.272461                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:52:57][base_learner.py:360][INFO] [RANK0]: === Training Iteration 1500 Result ===
[2025-08-15 07:52:57][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 20.904807               | 36.875551      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.901211        | 8.277392        | 13.756456      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.794752                  | 11.754159                 | 0.524542            | 122.419927         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.376894          | 266.286165       | 5.072090              | 237.920511           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.478151                      | 14.157224                    | 0.286946                        |
+-------+-------------------------------+------------------------------+---------------------------------+

[2025-08-15 07:55:32][base_learner.py:360][INFO] [RANK0]: === Training Iteration 1600 Result ===
[2025-08-15 07:55:32][learner_hook.py:228][INFO] 
+-------+------------------------------+------------+-------------------------+----------------+
| Name  | collect_mcts_temperature_avg | cur_lr_avg | weighted_total_loss_avg | total_loss_avg |
+-------+------------------------------+------------+-------------------------+----------------+
| Value | 1.000000                     | 0.003000   | 22.437393               | 36.559711      |
+-------+------------------------------+------------+-------------------------+----------------+
+-------+-----------------+-----------------+----------------+----------------------+
| Name  | policy_loss_avg | reward_loss_avg | value_loss_avg | consistency_loss_avg |
+-------+-----------------+-----------------+----------------+----------------------+
| Value | 7.772583        | 8.133403        | 13.689696      | 0.000000             |
+-------+-----------------+-----------------+----------------+----------------------+
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Name  | afterstate_policy_loss_avg | afterstate_value_loss_avg | commitment_loss_avg | value_priority_avg |
+-------+----------------------------+---------------------------+---------------------+--------------------+
| Value | 13.785478                  | 11.678814                 | 0.526121            | 125.460940         |
+-------+----------------------------+---------------------------+---------------------+--------------------+
+-------+-------------------+------------------+-----------------------+----------------------+
| Name  | target_reward_avg | target_value_avg | predicted_rewards_avg | predicted_values_avg |
+-------+-------------------+------------------+-----------------------+----------------------+
| Value | 8.017993          | 263.982691       | 4.981028              | 236.662218           |
+-------+-------------------+------------------+-----------------------+----------------------+
+-------+-------------------------------+------------------------------+---------------------------------+
| Name  | transformed_target_reward_avg | transformed_target_value_avg | total_grad_norm_before_clip_avg |
+-------+-------------------------------+------------------------------+---------------------------------+
| Value | 1.432952                      | 13.982869                    | 0.334674                        |
+-------+-------------------------------+------------------------------+---------------------------------+

