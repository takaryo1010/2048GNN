[2025-08-05 14:12:57][base_learner.py:360][INFO] [RANK0]: DI-engine DRL Policy
MuZeroModel(
  (representation_network): RepresentationNetwork(
    (conv): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (resblocks): ModuleList(
      (0): ResBlock(
        (act): ReLU(inplace=True)
        (conv1): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (conv2): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (activation): ReLU(inplace=True)
  )
  (dynamics_network): DynamicsNetwork(
    (conv): Conv2d(68, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (norm_common): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (resblocks): ModuleList(
      (0): ResBlock(
        (act): ReLU(inplace=True)
        (conv1): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (conv2): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (conv1x1_reward): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
    (norm_reward): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc_reward_head): Sequential(
      (0): Linear(in_features=256, out_features=32, bias=True)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=601, bias=True)
    )
    (activation): ReLU(inplace=True)
  )
  (prediction_network): PredictionNetwork(
    (resblocks): ModuleList(
      (0): ResBlock(
        (act): ReLU(inplace=True)
        (conv1): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (conv2): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (conv1x1_value): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
    (conv1x1_policy): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
    (norm_value): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm_policy): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU(inplace=True)
    (fc_value): Sequential(
      (0): Linear(in_features=256, out_features=32, bias=True)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=601, bias=True)
    )
    (fc_policy): Sequential(
      (0): Linear(in_features=256, out_features=32, bias=True)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=4, bias=True)
    )
  )
)
[2025-08-05 14:13:07][base_learner.py:360][INFO] [RANK0]: learner save ckpt in ./data_gat/game_2048_grid4_gat_ns100_upc200_rer0.0_bs512_heads4_seed0_250805_141249/ckpt/ckpt_best.pth.tar
[2025-08-05 14:13:42][base_learner.py:360][INFO] [RANK0]: === Training Iteration 0 Result ===
[2025-08-05 14:13:42][learner_hook.py:228][INFO] 
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Name  | analysis/dormant_ratio_encoder_avg | analysis/dormant_ratio_dynamics_avg | analysis/latent_state_l2_norms_avg | analysis/l2_norm_before_avg |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Value | 0.000000                           | 0.000000                            | 34.429710                          | 0.000000                    |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Name  | analysis/l2_norm_after_avg | analysis/grad_norm_before_avg | analysis/grad_norm_after_avg | collect_mcts_temperature_avg |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Value | 0.000000                   | 0.000000                      | 0.000000                     | 1.000000                     |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
+-------+------------+-------------------------+----------------+-----------------+
| Name  | cur_lr_avg | weighted_total_loss_avg | total_loss_avg | policy_loss_avg |
+-------+------------+-------------------------+----------------+-----------------+
| Value | 0.003000   | 49.602669               | 49.602669      | 8.011806        |
+-------+------------+-------------------------+----------------+-----------------+
+-------+--------------------+---------------------------+-----------------+----------------+
| Name  | policy_entropy_avg | target_policy_entropy_avg | reward_loss_avg | value_loss_avg |
+-------+--------------------+---------------------------+-----------------+----------------+
| Value | 1.386294           | 1.128022                  | 31.992973       | 38.391567      |
+-------+--------------------+---------------------------+-----------------+----------------+
+-------+----------------------+--------------------+-------------------+------------------+
| Name  | consistency_loss_avg | value_priority_avg | target_reward_avg | target_value_avg |
+-------+----------------------+--------------------+-------------------+------------------+
| Value | 0.000000             | 66.686981          | 6.781250          | 65.999672        |
+-------+----------------------+--------------------+-------------------+------------------+
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Name  | predicted_rewards_avg | predicted_values_avg | transformed_target_reward_avg | transformed_target_value_avg |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Value | -0.000006             | -0.000006            | 1.293516                      | 6.614449                     |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
+-------+---------------------------------+
| Name  | total_grad_norm_before_clip_avg |
+-------+---------------------------------+
| Value | 1.321278                        |
+-------+---------------------------------+

[2025-08-05 14:13:42][base_learner.py:360][INFO] [RANK0]: learner save ckpt in ./data_gat/game_2048_grid4_gat_ns100_upc200_rer0.0_bs512_heads4_seed0_250805_141249/ckpt/iteration_0.pth.tar
[2025-08-05 14:14:00][base_learner.py:360][INFO] [RANK0]: === Training Iteration 100 Result ===
[2025-08-05 14:14:00][learner_hook.py:228][INFO] 
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Name  | analysis/dormant_ratio_encoder_avg | analysis/dormant_ratio_dynamics_avg | analysis/latent_state_l2_norms_avg | analysis/l2_norm_before_avg |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Value | 0.000000                           | 0.000000                            | 28.696967                          | 0.000000                    |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Name  | analysis/l2_norm_after_avg | analysis/grad_norm_before_avg | analysis/grad_norm_after_avg | collect_mcts_temperature_avg |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Value | 0.000000                   | 0.000000                      | 0.000000                     | 1.000000                     |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
+-------+------------+-------------------------+----------------+-----------------+
| Name  | cur_lr_avg | weighted_total_loss_avg | total_loss_avg | policy_loss_avg |
+-------+------------+-------------------------+----------------+-----------------+
| Value | 0.003000   | 18.817416               | 18.817416      | 7.907917        |
+-------+------------+-------------------------+----------------+-----------------+
+-------+--------------------+---------------------------+-----------------+----------------+
| Name  | policy_entropy_avg | target_policy_entropy_avg | reward_loss_avg | value_loss_avg |
+-------+--------------------+---------------------------+-----------------+----------------+
| Value | 1.365618           | 1.128070                  | 7.589132        | 13.281470      |
+-------+--------------------+---------------------------+-----------------+----------------+
+-------+----------------------+--------------------+-------------------+------------------+
| Name  | consistency_loss_avg | value_priority_avg | target_reward_avg | target_value_avg |
+-------+----------------------+--------------------+-------------------+------------------+
| Value | 0.000000             | 35.943134          | 7.002604          | 69.756719        |
+-------+----------------------+--------------------+-------------------+------------------+
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Name  | predicted_rewards_avg | predicted_values_avg | transformed_target_reward_avg | transformed_target_value_avg |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Value | 3.459974              | 48.525180            | 1.323411                      | 6.839391                     |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
+-------+---------------------------------+
| Name  | total_grad_norm_before_clip_avg |
+-------+---------------------------------+
| Value | 0.293814                        |
+-------+---------------------------------+

[2025-08-05 14:14:50][base_learner.py:360][INFO] [RANK0]: === Training Iteration 200 Result ===
[2025-08-05 14:14:50][learner_hook.py:228][INFO] 
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Name  | analysis/dormant_ratio_encoder_avg | analysis/dormant_ratio_dynamics_avg | analysis/latent_state_l2_norms_avg | analysis/l2_norm_before_avg |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Value | 0.000000                           | 0.000000                            | 26.443794                          | 0.000000                    |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Name  | analysis/l2_norm_after_avg | analysis/grad_norm_before_avg | analysis/grad_norm_after_avg | collect_mcts_temperature_avg |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Value | 0.000000                   | 0.000000                      | 0.000000                     | 1.000000                     |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
+-------+------------+-------------------------+----------------+-----------------+
| Name  | cur_lr_avg | weighted_total_loss_avg | total_loss_avg | policy_loss_avg |
+-------+------------+-------------------------+----------------+-----------------+
| Value | 0.003000   | 12.914957               | 12.914957      | 7.871585        |
+-------+------------+-------------------------+----------------+-----------------+
+-------+--------------------+---------------------------+-----------------+----------------+
| Name  | policy_entropy_avg | target_policy_entropy_avg | reward_loss_avg | value_loss_avg |
+-------+--------------------+---------------------------+-----------------+----------------+
| Value | 1.359319           | 1.116087                  | 3.294905        | 6.993869       |
+-------+--------------------+---------------------------+-----------------+----------------+
+-------+----------------------+--------------------+-------------------+------------------+
| Name  | consistency_loss_avg | value_priority_avg | target_reward_avg | target_value_avg |
+-------+----------------------+--------------------+-------------------+------------------+
| Value | 0.000000             | 9.394662           | 6.987334          | 93.907322        |
+-------+----------------------+--------------------+-------------------+------------------+
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Name  | predicted_rewards_avg | predicted_values_avg | transformed_target_reward_avg | transformed_target_value_avg |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Value | 5.607045              | 86.802911            | 1.320774                      | 8.162166                     |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
+-------+---------------------------------+
| Name  | total_grad_norm_before_clip_avg |
+-------+---------------------------------+
| Value | 0.664965                        |
+-------+---------------------------------+

[2025-08-05 14:15:06][base_learner.py:360][INFO] [RANK0]: === Training Iteration 300 Result ===
[2025-08-05 14:15:06][learner_hook.py:228][INFO] 
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Name  | analysis/dormant_ratio_encoder_avg | analysis/dormant_ratio_dynamics_avg | analysis/latent_state_l2_norms_avg | analysis/l2_norm_before_avg |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Value | 0.000000                           | 0.000000                            | 25.314724                          | 0.000000                    |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Name  | analysis/l2_norm_after_avg | analysis/grad_norm_before_avg | analysis/grad_norm_after_avg | collect_mcts_temperature_avg |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Value | 0.000000                   | 0.000000                      | 0.000000                     | 1.000000                     |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
+-------+------------+-------------------------+----------------+-----------------+
| Name  | cur_lr_avg | weighted_total_loss_avg | total_loss_avg | policy_loss_avg |
+-------+------------+-------------------------+----------------+-----------------+
| Value | 0.003000   | 9.604211                | 9.604211       | 6.834424        |
+-------+------------+-------------------------+----------------+-----------------+
+-------+--------------------+---------------------------+-----------------+----------------+
| Name  | policy_entropy_avg | target_policy_entropy_avg | reward_loss_avg | value_loss_avg |
+-------+--------------------+---------------------------+-----------------+----------------+
| Value | 1.184012           | 1.001777                  | 1.450260        | 5.278109       |
+-------+--------------------+---------------------------+-----------------+----------------+
+-------+----------------------+--------------------+-------------------+------------------+
| Name  | consistency_loss_avg | value_priority_avg | target_reward_avg | target_value_avg |
+-------+----------------------+--------------------+-------------------+------------------+
| Value | 0.000000             | 6.403270           | 6.954664          | 140.962065       |
+-------+----------------------+--------------------+-------------------+------------------+
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Name  | predicted_rewards_avg | predicted_values_avg | transformed_target_reward_avg | transformed_target_value_avg |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Value | 6.602483              | 136.851794           | 1.330688                      | 10.261334                    |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
+-------+---------------------------------+
| Name  | total_grad_norm_before_clip_avg |
+-------+---------------------------------+
| Value | 0.572699                        |
+-------+---------------------------------+

[2025-08-05 14:15:48][base_learner.py:360][INFO] [RANK0]: === Training Iteration 400 Result ===
[2025-08-05 14:15:48][learner_hook.py:228][INFO] 
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Name  | analysis/dormant_ratio_encoder_avg | analysis/dormant_ratio_dynamics_avg | analysis/latent_state_l2_norms_avg | analysis/l2_norm_before_avg |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Value | 0.000000                           | 0.000000                            | 24.828401                          | 0.000000                    |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Name  | analysis/l2_norm_after_avg | analysis/grad_norm_before_avg | analysis/grad_norm_after_avg | collect_mcts_temperature_avg |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Value | 0.000000                   | 0.000000                      | 0.000000                     | 1.000000                     |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
+-------+------------+-------------------------+----------------+-----------------+
| Name  | cur_lr_avg | weighted_total_loss_avg | total_loss_avg | policy_loss_avg |
+-------+------------+-------------------------+----------------+-----------------+
| Value | 0.003000   | 9.646088                | 9.646088       | 6.783867        |
+-------+------------+-------------------------+----------------+-----------------+
+-------+--------------------+---------------------------+-----------------+----------------+
| Name  | policy_entropy_avg | target_policy_entropy_avg | reward_loss_avg | value_loss_avg |
+-------+--------------------+---------------------------+-----------------+----------------+
| Value | 1.168617           | 0.996908                  | 1.710874        | 4.605388       |
+-------+--------------------+---------------------------+-----------------+----------------+
+-------+----------------------+--------------------+-------------------+------------------+
| Name  | consistency_loss_avg | value_priority_avg | target_reward_avg | target_value_avg |
+-------+----------------------+--------------------+-------------------+------------------+
| Value | 0.000000             | 4.812300           | 6.888258          | 161.215569       |
+-------+----------------------+--------------------+-------------------+------------------+
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Name  | predicted_rewards_avg | predicted_values_avg | transformed_target_reward_avg | transformed_target_value_avg |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Value | 6.806227              | 159.412333           | 1.319926                      | 11.022541                    |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
+-------+---------------------------------+
| Name  | total_grad_norm_before_clip_avg |
+-------+---------------------------------+
| Value | 0.532145                        |
+-------+---------------------------------+

[2025-08-05 14:16:00][base_learner.py:360][INFO] [RANK0]: === Training Iteration 500 Result ===
[2025-08-05 14:16:00][learner_hook.py:228][INFO] 
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Name  | analysis/dormant_ratio_encoder_avg | analysis/dormant_ratio_dynamics_avg | analysis/latent_state_l2_norms_avg | analysis/l2_norm_before_avg |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Value | 0.000000                           | 0.000000                            | 24.578840                          | 0.000000                    |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Name  | analysis/l2_norm_after_avg | analysis/grad_norm_before_avg | analysis/grad_norm_after_avg | collect_mcts_temperature_avg |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Value | 0.000000                   | 0.000000                      | 0.000000                     | 1.000000                     |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
+-------+------------+-------------------------+----------------+-----------------+
| Name  | cur_lr_avg | weighted_total_loss_avg | total_loss_avg | policy_loss_avg |
+-------+------------+-------------------------+----------------+-----------------+
| Value | 0.003000   | 9.039330                | 9.039330       | 6.546953        |
+-------+------------+-------------------------+----------------+-----------------+
+-------+--------------------+---------------------------+-----------------+----------------+
| Name  | policy_entropy_avg | target_policy_entropy_avg | reward_loss_avg | value_loss_avg |
+-------+--------------------+---------------------------+-----------------+----------------+
| Value | 1.138382           | 0.967546                  | 1.276485        | 4.863567       |
+-------+--------------------+---------------------------+-----------------+----------------+
+-------+----------------------+--------------------+-------------------+------------------+
| Name  | consistency_loss_avg | value_priority_avg | target_reward_avg | target_value_avg |
+-------+----------------------+--------------------+-------------------+------------------+
| Value | 0.000000             | 5.544541           | 6.600024          | 173.410461       |
+-------+----------------------+--------------------+-------------------+------------------+
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Name  | predicted_rewards_avg | predicted_values_avg | transformed_target_reward_avg | transformed_target_value_avg |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Value | 6.552143              | 170.439615           | 1.289049                      | 11.471013                    |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
+-------+---------------------------------+
| Name  | total_grad_norm_before_clip_avg |
+-------+---------------------------------+
| Value | 0.379719                        |
+-------+---------------------------------+

[2025-08-05 14:17:03][base_learner.py:360][INFO] [RANK0]: === Training Iteration 600 Result ===
[2025-08-05 14:17:03][learner_hook.py:228][INFO] 
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Name  | analysis/dormant_ratio_encoder_avg | analysis/dormant_ratio_dynamics_avg | analysis/latent_state_l2_norms_avg | analysis/l2_norm_before_avg |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
| Value | 0.000000                           | 0.000000                            | 24.307719                          | 0.000000                    |
+-------+------------------------------------+-------------------------------------+------------------------------------+-----------------------------+
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Name  | analysis/l2_norm_after_avg | analysis/grad_norm_before_avg | analysis/grad_norm_after_avg | collect_mcts_temperature_avg |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
| Value | 0.000000                   | 0.000000                      | 0.000000                     | 1.000000                     |
+-------+----------------------------+-------------------------------+------------------------------+------------------------------+
+-------+------------+-------------------------+----------------+-----------------+
| Name  | cur_lr_avg | weighted_total_loss_avg | total_loss_avg | policy_loss_avg |
+-------+------------+-------------------------+----------------+-----------------+
| Value | 0.003000   | 9.486901                | 9.486901       | 6.533472        |
+-------+------------+-------------------------+----------------+-----------------+
+-------+--------------------+---------------------------+-----------------+----------------+
| Name  | policy_entropy_avg | target_policy_entropy_avg | reward_loss_avg | value_loss_avg |
+-------+--------------------+---------------------------+-----------------+----------------+
| Value | 1.127390           | 0.966703                  | 1.725585        | 4.911378       |
+-------+--------------------+---------------------------+-----------------+----------------+
+-------+----------------------+--------------------+-------------------+------------------+
| Name  | consistency_loss_avg | value_priority_avg | target_reward_avg | target_value_avg |
+-------+----------------------+--------------------+-------------------+------------------+
| Value | 0.000000             | 6.917869           | 6.762547          | 192.817967       |
+-------+----------------------+--------------------+-------------------+------------------+
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Name  | predicted_rewards_avg | predicted_values_avg | transformed_target_reward_avg | transformed_target_value_avg |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
| Value | 6.673341              | 190.115123           | 1.307102                      | 12.115533                    |
+-------+-----------------------+----------------------+-------------------------------+------------------------------+
+-------+---------------------------------+
| Name  | total_grad_norm_before_clip_avg |
+-------+---------------------------------+
| Value | 0.487702                        |
+-------+---------------------------------+

