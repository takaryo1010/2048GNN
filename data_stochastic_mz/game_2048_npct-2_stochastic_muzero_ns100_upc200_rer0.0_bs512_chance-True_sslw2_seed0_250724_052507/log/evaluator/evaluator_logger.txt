[2025-07-24 05:25:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 108.0, current episode: 1
[2025-07-24 05:25:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 156.0, current episode: 2
[2025-07-24 05:25:35][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 248.0, current episode: 3
[2025-07-24 05:25:35][muzero_evaluator.py:471][INFO] 
+-------+------------+---------------------+---------------+---------------+
| Name  | train_iter | ckpt_name           | episode_count | envstep_count |
+-------+------------+---------------------+---------------+---------------+
| Value | 0.000000   | iteration_0.pth.tar | 3.000000      | 127.000000    |
+-------+------------+---------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 42.333333               | 25.200807     | 5.039521            | 0.119044             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 170.666667  | 58.088056  | 248.000000 | 108.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [156.0, 108.0, 248.0] | 170.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 05:48:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 244.0, current episode: 1
[2025-07-24 05:48:08][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 284.0, current episode: 2
[2025-07-24 05:48:12][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 476.0, current episode: 3
[2025-07-24 05:48:12][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 2000.000000 | iteration_2000.pth.tar | 3.000000      | 173.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 57.666667               | 35.853984     | 4.825126            | 0.083673             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 334.666667  | 101.263134 | 476.000000 | 244.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [244.0, 284.0, 476.0] | 334.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 06:10:50][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 516.0, current episode: 1
[2025-07-24 06:10:57][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 708.0, current episode: 2
[2025-07-24 06:10:57][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 712.0, current episode: 3
[2025-07-24 06:10:57][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 4000.000000 | iteration_4000.pth.tar | 3.000000      | 264.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 88.000000               | 52.832262     | 4.996947            | 0.056783             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 645.333333  | 91.467055  | 712.000000 | 516.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [708.0, 516.0, 712.0] | 645.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 06:34:22][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 248.0, current episode: 1
[2025-07-24 06:34:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 612.0, current episode: 2
[2025-07-24 06:34:39][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 780.0, current episode: 3
[2025-07-24 06:34:39][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 6000.000000 | iteration_6000.pth.tar | 3.000000      | 231.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 77.000000               | 47.780449     | 4.834613            | 0.062787             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 546.666667  | 222.047042 | 780.000000 | 248.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [612.0, 780.0, 248.0] | 546.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 06:57:14][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 480.0, current episode: 1
[2025-07-24 06:57:22][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 668.0, current episode: 2
[2025-07-24 06:57:22][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 668.0, current episode: 3
[2025-07-24 06:57:22][muzero_evaluator.py:471][INFO] 
+-------+-------------+------------------------+---------------+---------------+
| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |
+-------+-------------+------------------------+---------------+---------------+
| Value | 8000.000000 | iteration_8000.pth.tar | 3.000000      | 253.000000    |
+-------+-------------+------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 84.333333               | 51.698434     | 4.893765            | 0.058029             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 605.333333  | 88.624050  | 668.000000 | 480.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [668.0, 480.0, 668.0] | 605.333333               |
+-------+-----------------------+--------------------------+

[2025-07-24 07:21:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 504.0, current episode: 1
[2025-07-24 07:21:25][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 1060.0, current episode: 2
[2025-07-24 07:21:35][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 1492.0, current episode: 3
[2025-07-24 07:21:35][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 10000.000000 | iteration_10000.pth.tar | 3.000000      | 342.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 114.000000              | 71.311437     | 4.795865            | 0.042069             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min |
+-------+-------------+------------+-------------+------------+
| Value | 1018.666667 | 404.406836 | 1492.000000 | 504.000000 |
+-------+-------------+------------+-------------+------------+
+-------+-------------------------+--------------------------+
| Name  | eval_episode_return     | eval_episode_return_mean |
+-------+-------------------------+--------------------------+
| Value | [1060.0, 504.0, 1492.0] | 1018.666667              |
+-------+-------------------------+--------------------------+

[2025-07-24 07:45:59][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 480.0, current episode: 1
[2025-07-24 07:46:00][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 592.0, current episode: 2
[2025-07-24 07:46:02][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 628.0, current episode: 3
[2025-07-24 07:46:02][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 12000.000000 | iteration_12000.pth.tar | 3.000000      | 246.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 82.000000               | 46.275281     | 5.316013            | 0.064829             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 566.666667  | 63.020279  | 628.000000 | 480.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [628.0, 480.0, 592.0] | 566.666667               |
+-------+-----------------------+--------------------------+

[2025-07-24 08:10:57][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 632.0, current episode: 1
[2025-07-24 08:11:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 820.0, current episode: 2
[2025-07-24 08:11:08][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 1100.0, current episode: 3
[2025-07-24 08:11:08][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 14000.000000 | iteration_14000.pth.tar | 3.000000      | 306.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 102.000000              | 60.183937     | 5.084413            | 0.049847             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min |
+-------+-------------+------------+-------------+------------+
| Value | 850.666667  | 192.286823 | 1100.000000 | 632.000000 |
+-------+-------------+------------+-------------+------------+
+-------+------------------------+--------------------------+
| Name  | eval_episode_return    | eval_episode_return_mean |
+-------+------------------------+--------------------------+
| Value | [820.0, 1100.0, 632.0] | 850.666667               |
+-------+------------------------+--------------------------+

[2025-07-24 08:36:35][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 720.0, current episode: 1
[2025-07-24 08:36:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 896.0, current episode: 2
[2025-07-24 08:36:37][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 760.0, current episode: 3
[2025-07-24 08:36:37][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 16000.000000 | iteration_16000.pth.tar | 3.000000      | 273.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 91.000000               | 53.734223     | 5.080561            | 0.055830             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 792.000000  | 75.330383  | 896.000000 | 720.000000 |
+-------+-------------+------------+------------+------------+
+-------+-----------------------+--------------------------+
| Name  | eval_episode_return   | eval_episode_return_mean |
+-------+-----------------------+--------------------------+
| Value | [720.0, 760.0, 896.0] | 792.000000               |
+-------+-----------------------+--------------------------+

[2025-07-24 09:02:15][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 664.0, current episode: 1
[2025-07-24 09:02:21][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 756.0, current episode: 2
[2025-07-24 09:02:25][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 1096.0, current episode: 3
[2025-07-24 09:02:25][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 18000.000000 | iteration_18000.pth.tar | 3.000000      | 310.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 103.333333              | 60.125719     | 5.155864            | 0.049895             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min |
+-------+-------------+------------+-------------+------------+
| Value | 838.666667  | 185.797979 | 1096.000000 | 664.000000 |
+-------+-------------+------------+-------------+------------+
+-------+------------------------+--------------------------+
| Name  | eval_episode_return    | eval_episode_return_mean |
+-------+------------------------+--------------------------+
| Value | [1096.0, 664.0, 756.0] | 838.666667               |
+-------+------------------------+--------------------------+

[2025-07-24 09:27:46][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 588.0, current episode: 1
[2025-07-24 09:28:16][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 1368.0, current episode: 2
[2025-07-24 09:28:34][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 2368.0, current episode: 3
[2025-07-24 09:28:34][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 20000.000000 | iteration_20000.pth.tar | 3.000000      | 425.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 141.666667              | 93.530305     | 4.543982            | 0.032075             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min |
+-------+-------------+------------+-------------+------------+
| Value | 1441.333333 | 728.529722 | 2368.000000 | 588.000000 |
+-------+-------------+------------+-------------+------------+
+-------+-------------------------+--------------------------+
| Name  | eval_episode_return     | eval_episode_return_mean |
+-------+-------------------------+--------------------------+
| Value | [588.0, 2368.0, 1368.0] | 1441.333333              |
+-------+-------------------------+--------------------------+

[2025-07-24 09:56:20][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 904.0, current episode: 1
[2025-07-24 09:56:33][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 1392.0, current episode: 2
[2025-07-24 09:56:43][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 1724.0, current episode: 3
[2025-07-24 09:56:43][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 22000.000000 | iteration_22000.pth.tar | 3.000000      | 432.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 144.000000              | 88.629688     | 4.874213            | 0.033849             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min |
+-------+-------------+------------+-------------+------------+
| Value | 1340.000000 | 336.776880 | 1724.000000 | 904.000000 |
+-------+-------------+------------+-------------+------------+
+-------+-------------------------+--------------------------+
| Name  | eval_episode_return     | eval_episode_return_mean |
+-------+-------------------------+--------------------------+
| Value | [1392.0, 1724.0, 904.0] | 1340.000000              |
+-------+-------------------------+--------------------------+

[2025-07-24 10:24:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 844.0, current episode: 1
[2025-07-24 10:24:53][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 1048.0, current episode: 2
[2025-07-24 10:24:57][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 1328.0, current episode: 3
[2025-07-24 10:24:57][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 24000.000000 | iteration_24000.pth.tar | 3.000000      | 382.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 127.333333              | 78.137250     | 4.888833            | 0.038394             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min |
+-------+-------------+------------+-------------+------------+
| Value | 1073.333333 | 198.402509 | 1328.000000 | 844.000000 |
+-------+-------------+------------+-------------+------------+
+-------+-------------------------+--------------------------+
| Name  | eval_episode_return     | eval_episode_return_mean |
+-------+-------------------------+--------------------------+
| Value | [1048.0, 844.0, 1328.0] | 1073.333333              |
+-------+-------------------------+--------------------------+

[2025-07-24 10:50:18][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 704.0, current episode: 1
[2025-07-24 10:50:42][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 1376.0, current episode: 2
[2025-07-24 10:51:10][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 2964.0, current episode: 3
[2025-07-24 10:51:10][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 26000.000000 | iteration_26000.pth.tar | 3.000000      | 485.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 161.666667              | 106.003180    | 4.575334            | 0.028301             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min |
+-------+-------------+------------+-------------+------------+
| Value | 1681.333333 | 947.565770 | 2964.000000 | 704.000000 |
+-------+-------------+------------+-------------+------------+
+-------+-------------------------+--------------------------+
| Name  | eval_episode_return     | eval_episode_return_mean |
+-------+-------------------------+--------------------------+
| Value | [2964.0, 704.0, 1376.0] | 1681.333333              |
+-------+-------------------------+--------------------------+

[2025-07-24 11:17:16][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 408.0, current episode: 1
[2025-07-24 11:17:47][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 1296.0, current episode: 2
[2025-07-24 11:17:47][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 1336.0, current episode: 3
[2025-07-24 11:17:47][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 28000.000000 | iteration_28000.pth.tar | 3.000000      | 330.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 110.000000              | 67.954930     | 4.856160            | 0.044147             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min |
+-------+-------------+------------+-------------+------------+
| Value | 1013.333333 | 428.346692 | 1336.000000 | 408.000000 |
+-------+-------------+------------+-------------+------------+
+-------+-------------------------+--------------------------+
| Name  | eval_episode_return     | eval_episode_return_mean |
+-------+-------------------------+--------------------------+
| Value | [1336.0, 408.0, 1296.0] | 1013.333333              |
+-------+-------------------------+--------------------------+

[2025-07-24 11:44:46][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 488.0, current episode: 1
[2025-07-24 11:45:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 916.0, current episode: 2
[2025-07-24 11:45:10][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 1172.0, current episode: 3
[2025-07-24 11:45:10][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 30000.000000 | iteration_30000.pth.tar | 3.000000      | 309.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 103.000000              | 62.704859     | 4.927848            | 0.047843             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min |
+-------+-------------+------------+-------------+------------+
| Value | 858.666667  | 282.169374 | 1172.000000 | 488.000000 |
+-------+-------------+------------+-------------+------------+
+-------+------------------------+--------------------------+
| Name  | eval_episode_return    | eval_episode_return_mean |
+-------+------------------------+--------------------------+
| Value | [916.0, 488.0, 1172.0] | 858.666667               |
+-------+------------------------+--------------------------+

[2025-07-24 12:14:07][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 2252.0, current episode: 1
[2025-07-24 12:14:09][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 2304.0, current episode: 2
[2025-07-24 12:14:24][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 2908.0, current episode: 3
[2025-07-24 12:14:24][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 32000.000000 | iteration_32000.pth.tar | 3.000000      | 631.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 210.333333              | 127.434234    | 4.951574            | 0.023542             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+-------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min  |
+-------+-------------+------------+-------------+-------------+
| Value | 2488.000000 | 297.742618 | 2908.000000 | 2252.000000 |
+-------+-------------+------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [2908.0, 2304.0, 2252.0] | 2488.000000              |
+-------+--------------------------+--------------------------+

[2025-07-24 12:45:26][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 3044.0, current episode: 1
[2025-07-24 12:45:26][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 2912.0, current episode: 2
[2025-07-24 12:45:32][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 3260.0, current episode: 3
[2025-07-24 12:45:32][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 34000.000000 | iteration_34000.pth.tar | 3.000000      | 765.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 255.000000              | 150.008250    | 5.099720            | 0.019999             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+-------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min  |
+-------+-------------+------------+-------------+-------------+
| Value | 3072.000000 | 143.443369 | 3260.000000 | 2912.000000 |
+-------+-------------+------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [3044.0, 3260.0, 2912.0] | 3072.000000              |
+-------+--------------------------+--------------------------+

[2025-07-24 13:15:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 1512.0, current episode: 1
[2025-07-24 13:16:03][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 2816.0, current episode: 2
[2025-07-24 13:16:12][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 3252.0, current episode: 3
[2025-07-24 13:16:12][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 36000.000000 | iteration_36000.pth.tar | 3.000000      | 653.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 217.666667              | 127.209836    | 5.133251            | 0.023583             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+-------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min  |
+-------+-------------+------------+-------------+-------------+
| Value | 2526.666667 | 739.227224 | 3252.000000 | 1512.000000 |
+-------+-------------+------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [2816.0, 3252.0, 1512.0] | 2526.666667              |
+-------+--------------------------+--------------------------+

[2025-07-24 13:48:11][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 1992.0, current episode: 1
[2025-07-24 13:48:31][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 2772.0, current episode: 2
[2025-07-24 13:48:37][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 3012.0, current episode: 3
[2025-07-24 13:48:37][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 38000.000000 | iteration_38000.pth.tar | 3.000000      | 661.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 220.333333              | 125.723398    | 5.257573            | 0.023862             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+-------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min  |
+-------+-------------+------------+-------------+-------------+
| Value | 2592.000000 | 435.430821 | 3012.000000 | 1992.000000 |
+-------+-------------+------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [1992.0, 2772.0, 3012.0] | 2592.000000              |
+-------+--------------------------+--------------------------+

[2025-07-24 14:20:30][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 1508.0, current episode: 1
[2025-07-24 14:21:13][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 3100.0, current episode: 2
[2025-07-24 14:21:18][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 3288.0, current episode: 3
[2025-07-24 14:21:18][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 40000.000000 | iteration_40000.pth.tar | 3.000000      | 694.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 231.333333              | 139.418422    | 4.977821            | 0.021518             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+-------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min  |
+-------+-------------+------------+-------------+-------------+
| Value | 2632.000000 | 798.485233 | 3288.000000 | 1508.000000 |
+-------+-------------+------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [3288.0, 1508.0, 3100.0] | 2632.000000              |
+-------+--------------------------+--------------------------+

[2025-07-24 14:58:41][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 3232.0, current episode: 1
[2025-07-24 14:58:53][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 3740.0, current episode: 2
[2025-07-24 14:59:18][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 5444.0, current episode: 3
[2025-07-24 14:59:18][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 42000.000000 | iteration_42000.pth.tar | 3.000000      | 943.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 314.333333              | 184.812000    | 5.102483            | 0.016233             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+-------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min  |
+-------+-------------+------------+-------------+-------------+
| Value | 4138.666667 | 946.022316 | 5444.000000 | 3232.000000 |
+-------+-------------+------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [3232.0, 5444.0, 3740.0] | 4138.666667              |
+-------+--------------------------+--------------------------+

[2025-07-24 15:43:10][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 3620.0, current episode: 1
[2025-07-24 15:44:29][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 6792.0, current episode: 2
[2025-07-24 15:44:48][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 7796.0, current episode: 3
[2025-07-24 15:44:48][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 44000.000000 | iteration_44000.pth.tar | 3.000000      | 1323.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 441.000000              | 259.611188    | 5.096082            | 0.011556             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+-------------+-------------+-------------+
| Name  | reward_mean | reward_std  | reward_max  | reward_min  |
+-------+-------------+-------------+-------------+-------------+
| Value | 6069.333333 | 1779.780761 | 7796.000000 | 3620.000000 |
+-------+-------------+-------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [3620.0, 6792.0, 7796.0] | 6069.333333              |
+-------+--------------------------+--------------------------+

[2025-07-24 16:35:23][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 5128.0, current episode: 1
[2025-07-24 16:36:30][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 7900.0, current episode: 2
[2025-07-24 16:36:32][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 7904.0, current episode: 3
[2025-07-24 16:36:32][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 46000.000000 | iteration_46000.pth.tar | 3.000000      | 1496.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 498.666667              | 292.801375    | 5.109266            | 0.010246             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+-------------+-------------+-------------+
| Name  | reward_mean | reward_std  | reward_max  | reward_min  |
+-------+-------------+-------------+-------------+-------------+
| Value | 6977.333333 | 1307.677160 | 7904.000000 | 5128.000000 |
+-------+-------------+-------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [7904.0, 7900.0, 5128.0] | 6977.333333              |
+-------+--------------------------+--------------------------+

[2025-07-24 17:26:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 4892.0, current episode: 1
[2025-07-24 17:26:45][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 5448.0, current episode: 2
[2025-07-24 17:28:12][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 11720.0, current episode: 3
[2025-07-24 17:28:12][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 48000.000000 | iteration_48000.pth.tar | 3.000000      | 1450.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 483.333333              | 310.568094    | 4.668863            | 0.009660             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+-------------+--------------+-------------+
| Name  | reward_mean | reward_std  | reward_max   | reward_min  |
+-------+-------------+-------------+--------------+-------------+
| Value | 7353.333333 | 3096.031582 | 11720.000000 | 4892.000000 |
+-------+-------------+-------------+--------------+-------------+
+-------+---------------------------+--------------------------+
| Name  | eval_episode_return       | eval_episode_return_mean |
+-------+---------------------------+--------------------------+
| Value | [5448.0, 4892.0, 11720.0] | 7353.333333              |
+-------+---------------------------+--------------------------+

[2025-07-24 18:21:26][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 3404.0, current episode: 1
[2025-07-24 18:22:05][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 4856.0, current episode: 2
[2025-07-24 18:22:17][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 5736.0, current episode: 3
[2025-07-24 18:22:17][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 50000.000000 | iteration_50000.pth.tar | 3.000000      | 1061.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 353.666667              | 217.654109    | 4.874707            | 0.013783             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+-------------+-------------+
| Name  | reward_mean | reward_std | reward_max  | reward_min  |
+-------+-------------+------------+-------------+-------------+
| Value | 4665.333333 | 961.533960 | 5736.000000 | 3404.000000 |
+-------+-------------+------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [5736.0, 4856.0, 3404.0] | 4665.333333              |
+-------+--------------------------+--------------------------+

[2025-07-24 19:47:24][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 4500.0, current episode: 1
[2025-07-24 19:49:03][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 8060.0, current episode: 2
[2025-07-24 19:49:32][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 11204.0, current episode: 3
[2025-07-24 19:49:32][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 52000.000000 | iteration_52000.pth.tar | 3.000000      | 1560.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 520.000000              | 323.362594    | 4.824306            | 0.009278             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+-------------+--------------+-------------+
| Name  | reward_mean | reward_std  | reward_max   | reward_min  |
+-------+-------------+-------------+--------------+-------------+
| Value | 7921.333333 | 2738.652386 | 11204.000000 | 4500.000000 |
+-------+-------------+-------------+--------------+-------------+
+-------+---------------------------+--------------------------+
| Name  | eval_episode_return       | eval_episode_return_mean |
+-------+---------------------------+--------------------------+
| Value | [11204.0, 8060.0, 4500.0] | 7921.333333              |
+-------+---------------------------+--------------------------+

[2025-07-24 21:17:43][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 6116.0, current episode: 1
[2025-07-24 21:18:02][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 6644.0, current episode: 2
[2025-07-24 21:20:21][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 16284.0, current episode: 3
[2025-07-24 21:20:21][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 54000.000000 | iteration_54000.pth.tar | 3.000000      | 1818.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 606.000000              | 407.711000    | 4.459041            | 0.007358             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+-------------+--------------+-------------+
| Name  | reward_mean | reward_std  | reward_max   | reward_min  |
+-------+-------------+-------------+--------------+-------------+
| Value | 9681.333333 | 4673.763746 | 16284.000000 | 6116.000000 |
+-------+-------------+-------------+--------------+-------------+
+-------+---------------------------+--------------------------+
| Name  | eval_episode_return       | eval_episode_return_mean |
+-------+---------------------------+--------------------------+
| Value | [6116.0, 16284.0, 6644.0] | 9681.333333              |
+-------+---------------------------+--------------------------+

[2025-07-24 22:54:01][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 7324.0, current episode: 1
[2025-07-24 22:56:14][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 13200.0, current episode: 2
[2025-07-24 22:56:28][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 14000.0, current episode: 3
[2025-07-24 22:56:28][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 56000.000000 | iteration_56000.pth.tar | 3.000000      | 2123.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 707.666667              | 456.993187    | 4.645583            | 0.006565             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+--------------+-------------+--------------+-------------+
| Name  | reward_mean  | reward_std  | reward_max   | reward_min  |
+-------+--------------+-------------+--------------+-------------+
| Value | 11508.000000 | 2976.507125 | 14000.000000 | 7324.000000 |
+-------+--------------+-------------+--------------+-------------+
+-------+----------------------------+--------------------------+
| Name  | eval_episode_return        | eval_episode_return_mean |
+-------+----------------------------+--------------------------+
| Value | [13200.0, 7324.0, 14000.0] | 11508.000000             |
+-------+----------------------------+--------------------------+

[2025-07-25 00:34:36][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 4368.0, current episode: 1
[2025-07-25 00:36:04][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 8160.0, current episode: 2
[2025-07-25 00:36:39][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 11656.0, current episode: 3
[2025-07-25 00:36:39][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 58000.000000 | iteration_58000.pth.tar | 3.000000      | 1645.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 548.333333              | 330.864781    | 4.971820            | 0.009067             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+-------------+--------------+-------------+
| Name  | reward_mean | reward_std  | reward_max   | reward_min  |
+-------+-------------+-------------+--------------+-------------+
| Value | 8061.333333 | 2976.131419 | 11656.000000 | 4368.000000 |
+-------+-------------+-------------+--------------+-------------+
+-------+---------------------------+--------------------------+
| Name  | eval_episode_return       | eval_episode_return_mean |
+-------+---------------------------+--------------------------+
| Value | [11656.0, 8160.0, 4368.0] | 8061.333333              |
+-------+---------------------------+--------------------------+

[2025-07-25 02:08:23][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 9384.0, current episode: 1
[2025-07-25 02:09:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 12788.0, current episode: 2
[2025-07-25 02:09:40][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 12544.0, current episode: 3
[2025-07-25 02:09:40][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 60000.000000 | iteration_60000.pth.tar | 3.000000      | 2127.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 709.000000              | 451.758531    | 4.708267            | 0.006641             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+--------------+-------------+--------------+-------------+
| Name  | reward_mean  | reward_std  | reward_max   | reward_min  |
+-------+--------------+-------------+--------------+-------------+
| Value | 11572.000000 | 1550.353078 | 12788.000000 | 9384.000000 |
+-------+--------------+-------------+--------------+-------------+
+-------+----------------------------+--------------------------+
| Name  | eval_episode_return        | eval_episode_return_mean |
+-------+----------------------------+--------------------------+
| Value | [12788.0, 12544.0, 9384.0] | 11572.000000             |
+-------+----------------------------+--------------------------+

[2025-07-25 03:55:02][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 1820.0, current episode: 1
[2025-07-25 03:55:23][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 2820.0, current episode: 2
[2025-07-25 03:56:55][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 7792.0, current episode: 3
[2025-07-25 03:56:55][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 62000.000000 | iteration_62000.pth.tar | 3.000000      | 962.000000    |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 320.666667              | 227.038344    | 4.237170            | 0.013214             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+-------------+-------------+-------------+
| Name  | reward_mean | reward_std  | reward_max  | reward_min  |
+-------+-------------+-------------+-------------+-------------+
| Value | 4144.000000 | 2611.631419 | 7792.000000 | 1820.000000 |
+-------+-------------+-------------+-------------+-------------+
+-------+--------------------------+--------------------------+
| Name  | eval_episode_return      | eval_episode_return_mean |
+-------+--------------------------+--------------------------+
| Value | [7792.0, 1820.0, 2820.0] | 4144.000000              |
+-------+--------------------------+--------------------------+

[2025-07-25 05:28:27][muzero_evaluator.py:390][INFO] [EVALUATOR]env 1 finish episode, final reward: 7312.0, current episode: 1
[2025-07-25 05:29:38][muzero_evaluator.py:390][INFO] [EVALUATOR]env 2 finish episode, final reward: 9612.0, current episode: 2
[2025-07-25 05:31:18][muzero_evaluator.py:390][INFO] [EVALUATOR]env 0 finish episode, final reward: 16880.0, current episode: 3
[2025-07-25 05:31:18][muzero_evaluator.py:471][INFO] 
+-------+--------------+-------------------------+---------------+---------------+
| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |
+-------+--------------+-------------------------+---------------+---------------+
| Value | 64000.000000 | iteration_64000.pth.tar | 3.000000      | 2171.000000   |
+-------+--------------+-------------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 723.666667              | 471.357875    | 4.605842            | 0.006365             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+--------------+-------------+--------------+-------------+
| Name  | reward_mean  | reward_std  | reward_max   | reward_min  |
+-------+--------------+-------------+--------------+-------------+
| Value | 11268.000000 | 4077.859569 | 16880.000000 | 7312.000000 |
+-------+--------------+-------------+--------------+-------------+
+-------+---------------------------+--------------------------+
| Name  | eval_episode_return       | eval_episode_return_mean |
+-------+---------------------------+--------------------------+
| Value | [16880.0, 7312.0, 9612.0] | 11268.000000             |
+-------+---------------------------+--------------------------+

